"""
AI Service - Integracja z modelem jÄ™zykowym (LLM) poprzez Ollama
"""
import json
import asyncio
import uuid
import hashlib
import functools
from typing import Dict, List, Any, Optional, cast
from datetime import datetime
import logging

import ollama
from app.core.config import settings

# --- POCZÄ„TEK ZMIAN ---

# Dynamiczne tworzenie nagÅ‚Ã³wkÃ³w do autoryzacji w Ollama Turbo
headers = {}
if settings.OLLAMA_API_KEY:
    headers['Authorization'] = f'Bearer {settings.OLLAMA_API_KEY}'

# Inicjalizacja klienta z hostem i nagÅ‚Ã³wkami
client = ollama.Client(
    host=settings.OLLAMA_API_URL,
    headers=headers
)

# --- KONIEC ZMIAN ---

from ollama import Client
from pydantic import ValidationError

from app.schemas.interaction import InteractionResponse
from app.models.domain import Client as DomainClient, Session, Interaction
from .qdrant_service import QdrantService

logger = logging.getLogger(__name__)


# Prompt psychometryczny dla ModuÅ‚u 2: Zintegrowana Analiza Psychometryczna
PSYCHOMETRIC_SYSTEM_PROMPT = """
JesteÅ› ekspertem w dziedzinie psychologii sprzedaÅ¼y i lingwistyki. Twoim zadaniem jest przeanalizowaÄ‡ poniÅ¼szÄ… transkrypcjÄ™ rozmowy sprzedaÅ¼owej i stworzyÄ‡ szczegÃ³Å‚owy profil psychometryczny klienta. Wynik przedstaw WYÅÄ„CZNIE jako JSON zgodny z podanÄ… strukturÄ….

KROKI ANALIZY:

1. **Analiza Big Five:** OceÅ„ klienta w 5 wymiarach osobowoÅ›ci (0-10). Dla kaÅ¼dej cechy podaj UZASADNIENIE (rationale) z cytatami z rozmowy oraz STRATEGIÄ˜ sprzedaÅ¼owÄ… dostosowanÄ… do tej cechy.

2. **Analiza DISC:** OceÅ„ dominujÄ…cy styl zachowania klienta (0-10) w 4 wymiarach. Dla kaÅ¼dej cechy podaj UZASADNIENIE z przykÅ‚adami oraz STRATEGIÄ˜ sprzedaÅ¼owÄ….

3. **Analiza WartoÅ›ci Schwartza:** Zidentyfikuj, ktÃ³re z kluczowych wartoÅ›ci (BezpieczeÅ„stwo, WÅ‚adza, OsiÄ…gniÄ™cia, Hedonizm, Stymulacja, Samostanowienie, Uniwersalizm, Å»yczliwoÅ›Ä‡, Tradycja, Przystosowanie) sÄ… obecne w wypowiedziach klienta. Dla kaÅ¼dej podaj UZASADNIENIE i STRATEGIÄ˜.

ENHANCED GUIDELINES - Precyzyjna Analiza:

BIG FIVE - WskazÃ³wki Specyficzne:
- Openness (0-10): Czy klient pyta o nowe technologie, innowacje, funkcje przyszÅ‚oÅ›ci?
- Conscientiousness (0-10): Czy wymaga szczegÃ³Å‚Ã³w, danych, planuje dÅ‚ugoterminowo?
- Extraversion (0-10): Czy mÃ³wi o innych ludziach, statusie, wraÅ¼eniu na otoczenie?
- Agreeableness (0-10): Czy unika konfrontacji, szuka konsensusu, jest uprzejmy?
- Neuroticism (0-10): Czy wyraÅ¼a obawy, stres, niepewnoÅ›Ä‡, potrzebÄ™ bezpieczeÅ„stwa?

DISC - WskazÃ³wki Behawioralne:
- Dominance (0-10): Czy jest bezpoÅ›redni, decyzyjny, chce kontrolowaÄ‡ proces?
- Influence (0-10): Czy jest towarzyski, perswazyjny, opowiada historie?
- Steadiness (0-10): Czy jest cierpliwy, lojalny, szuka stabilnoÅ›ci?
- Compliance (0-10): Czy jest analityczny, systematyczny, potrzebuje dowodÃ³w?

SCHWARTZ VALUES - Kluczowe Motywatory:
- BezpieczeÅ„stwo: Gwarancje, koszty, niezawodnoÅ›Ä‡
- WÅ‚adza: Status, prestiÅ¼, kontrola, wpÅ‚yw na innych
- OsiÄ…gniÄ™cia: Sukces, kompetencje, wyniki, efektywnoÅ›Ä‡
- Hedonizm: PrzyjemnoÅ›Ä‡, komfort, luksus
- Stymulacja: NowoÅ›Ä‡, wyzwania, ekscytacja
- Samostanowienie: NiezaleÅ¼noÅ›Ä‡, autonomia, wÅ‚asne decyzje
- Uniwersalizm: Ekologia, dobro ogÃ³Å‚u, sprawiedliwoÅ›Ä‡
- Å»yczliwoÅ›Ä‡: Troska o innych, relacje, wspÃ³Å‚praca
- Tradycja: Szacunek dla kultury, stabilne wartoÅ›ci
- Przystosowanie: Dopasowanie do norm, uprzejmoÅ›Ä‡

STRUKTURA WYJÅšCIOWA - zwrÃ³Ä‡ WYÅÄ„CZNIE ten JSON:
{
  "big_five": {
    "openness": { "score": 7, "rationale": "Klient wypowiedziaÅ‚: '[cytat z rozmowy]', co wskazuje na...", "strategy": "Skoncentruj siÄ™ na innowacyjnych cechach Tesla..." },
    "conscientiousness": { "score": 8, "rationale": "Z wypowiedzi '[cytat]' wynika...", "strategy": "Przedstaw szczegÃ³Å‚owe dane o ROI i TCO..." },
    "extraversion": { "score": 6, "rationale": "...", "strategy": "..." },
    "agreeableness": { "score": 5, "rationale": "...", "strategy": "..." },
    "neuroticism": { "score": 4, "rationale": "...", "strategy": "..." }
  },
  "disc": {
    "dominance": { "score": 6, "rationale": "Klient wykazuje cechy dominacji przez...", "strategy": "BÄ…dÅº bezpoÅ›redni, prezentuj fakty..." },
    "influence": { "score": 4, "rationale": "...", "strategy": "..." },
    "steadiness": { "score": 7, "rationale": "...", "strategy": "..." },
    "compliance": { "score": 8, "rationale": "...", "strategy": "..." }
  },
  "schwartz_values": [
    { "value_name": "BezpieczeÅ„stwo", "is_present": true, "rationale": "Klient wyraziÅ‚ obawy o...", "strategy": "PodkreÅ›l najwyÅ¼sze oceny bezpieczeÅ„stwa Tesla..." },
    { "value_name": "OsiÄ…gniÄ™cia", "is_present": false, "rationale": "Brak oznak zorientowania na sukces...", "strategy": "..." }
  ]
}

KLUCZOWE WYMAGANIA:
- KaÅ¼de uzasadnienie MUSI zawieraÄ‡ konkretne cytaty z rozmowy
- Strategie muszÄ… byÄ‡ praktyczne i gotowe do uÅ¼ycia przez sprzedawcÄ™ Tesla
- Oceny muszÄ… byÄ‡ realistyczne i oparte na faktycznych dowodach z tekstu
- JSON musi byÄ‡ poprawnie sformatowany (bez komentarzy)

JEÅšLI BRAK WYSTARCZAJÄ„CYCH DANYCH:
JeÅ›li rozmowa jest zbyt krÃ³tka lub nie zawiera wystarczajÄ…cych informacji do precyzyjnej analizy psychometrycznej, 
zamiast JSON zwrÃ³Ä‡:

{
  "insufficient_data": true,
  "probing_questions": [
    "Konkretne pytanie pomagajÄ…ce okreÅ›liÄ‡ Big Five",
    "Pytanie o styl komunikacji (DISC)",
    "Pytanie o motywacje i wartoÅ›ci (Schwartz)"
  ],
  "analysis_confidence": "low",
  "suggestions": "Co sprzedawca powinien sprawdziÄ‡ aby lepiej zrozumieÄ‡ psychologiÄ™ klienta"
}
"""

# Enhanced prompt dla Dwuetapowej Analizy Psychometrycznej
DUAL_STAGE_PSYCHOMETRIC_PROMPT = """
JesteÅ› ekspertem psychologii sprzedaÅ¼y prowadzÄ…cym DWUETAPOWÄ„ ANALIZÄ˜ klienta.

ETAP 1 - WSTÄ˜PNA ANALIZA:
1. Przeanalizuj dostÄ™pny tekst pod kÄ…tem Big Five, DISC i Schwartz
2. Dla kaÅ¼dego wymiaru oblicz wstÄ™pnÄ… ocenÄ™ i PEWNOÅšÄ† tej oceny (0-100%)
3. Oblicz OGÃ“LNÄ„ PEWNOÅšÄ† caÅ‚ej analizy jako Å›redniÄ… waÅ¼onÄ…

ETAP 2 - SAMOOCENA AI:
Zadaj sobie pytanie: "Czy na podstawie dostarczonych informacji mÃ³j poziom pewnoÅ›ci 
co do okreÅ›lonego profilu psychometrycznego jest wystarczajÄ…co wysoki (â‰¥75%)?"

JEÅšLI PEWNOÅšÄ† â‰¥ 75%:
ZwrÃ³Ä‡ peÅ‚nÄ… analizÄ™ psychometrycznÄ… bez dodatkowych pytaÅ„.

JEÅšLI PEWNOÅšÄ† < 75%:
Zidentyfikuj KONKRETNIE jakich informacji brakuje i wygeneruj 2-3 pytania A/B dla sprzedawcy.

STRUKTURA ODPOWIEDZI:
{
  "confidence_score": 85,
  "needs_clarification": false,
  "analysis_stage": "confirmed",
  "big_five": { ... },
  "disc": { ... },
  "schwartz_values": [ ... ],
  "clarifying_questions": [
    {
      "id": "q1_decision_style",
      "question": "Jak klient podejmuje decyzje?",
      "option_a": "Szybko, intuicyjnie", 
      "option_b": "Wolno, po szczegÃ³Å‚owej analizie",
      "psychological_target": "Conscientiousness vs Openness"
    }
  ]
}

KLUCZOWE: Pytania sÄ… dla SPRZEDAWCY o jego OBSERWACJE, nie do zadania klientowi!
"""

# Prompt dla Enhanced Response Generation z profilem psychometrycznym
PSYCHOLOGICALLY_INFORMED_RESPONSE_PROMPT = """
JesteÅ› ekspertem sprzedaÅ¼y Tesla generujÄ…cym PSYCHOLOGICZNIE DOSTOSOWANÄ„ odpowiedÅº.

Otrzymujesz POTWIERDZONY profil psychometryczny klienta i musisz wygenerowaÄ‡ 
sugerowanÄ… odpowiedÅº ktÃ³ra jest precyzyjnie dostosowana do jego psychologii.

UÅ»YJ PROFILU PSYCHOMETRYCZNEGO do:
1. Dostosowania tonu i stylu komunikacji (DISC)
2. Adresowania gÅ‚Ã³wnych motywatorÃ³w (Schwartz Values)  
3. Dostosowania poziomu szczegÃ³Å‚owoÅ›ci (Big Five - Conscientiousness)
4. UwzglÄ™dnienia obaw i lÄ™kÃ³w (Big Five - Neuroticism)

STRUKTURA ODPOWIEDZI:
{
  "quick_response": {
    "id": "qr_xxx",
    "text": "Psychologicznie dostosowana odpowiedÅº uwzglÄ™dniajÄ…ca profil klienta"
  },
  "psychological_reasoning": "Dlaczego ta odpowiedÅº jest dostosowana do profilu",
  "confidence_level": 95
}
"""


class AIService:
    """
    Tesla Co-Pilot AI Service - Elitarny ekspert sprzedaÅ¼y Tesli
    Integruje z modelem gpt-oss:120b poprzez Ollama Turbo Cloud
    
    MISJA: Absolutna lojalnoÅ›Ä‡ wobec marki Tesla. Zero kompromisÃ³w.
    
    Konfiguracja:
    - Host: https://ollama.com (chmura z akceleracjÄ… sprzÄ™towÄ…)
    - Autoryzacja: Bearer token z OLLAMA_API_KEY
    - Model: gpt-oss:120b
    """
    
    def __init__(self, qdrant_service: QdrantService):
        """
        Inicjalizacja AI Service z integracjÄ… bazy wiedzy (RAG) i performance optimization
        
        Args:
            qdrant_service: Instancja serwisu Qdrant do pobierania wiedzy kontekstowej
        """
        self.qdrant_service = qdrant_service
        self.model_name = settings.OLLAMA_MODEL  # UÅ¼ywamy konfiguracji z settings
        self.max_retries = 3
        self.timeout_seconds = 60

        # ğŸš€ PRIORYTET 4: Performance optimization caches
        self._holistic_synthesis_cache = {}  # Cache dla DNA Klienta  
        self._sales_indicators_cache = {}    # Cache dla wskaÅºnikÃ³w sprzedaÅ¼owych
        self._cache_max_size = 128           # Maksymalna liczba cached entries
        self._cache_ttl_seconds = 3600       # 1 godzina TTL

        # UÅ¼yj globalnego klienta zainicjalizowanego na poziomie moduÅ‚u
        self.client = client
        logger.info("âœ… Tesla Co-Pilot AI zostaÅ‚ pomyÅ›lnie skonfigurowany z integracjÄ… RAG i performance caching.")
    
    def _generate_cache_key(self, data: Dict[str, Any], prefix: str = "") -> str:
        """
        ğŸš€ PRIORYTET 4: Generuje kluczowy hash dla cache na podstawie danych wejÅ›ciowych
        
        Args:
            data: Dane do zhashowania (np. psychology profile, conversation history)
            prefix: Prefix dla klucza (np. "synthesis", "indicators")
            
        Returns:
            str: Unique cache key
        """
        # Konwertuj dane na stabilny JSON string
        json_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
        # Wygeneruj SHA256 hash
        hash_object = hashlib.sha256(json_str.encode('utf-8'))
        hash_hex = hash_object.hexdigest()[:16]  # Pierwsze 16 znakÃ³w wystarczÄ…
        
        return f"{prefix}:{hash_hex}" if prefix else hash_hex

    def _get_from_cache(self, cache_dict: dict, key: str) -> Optional[Dict[str, Any]]:
        """
        ğŸš€ PRIORYTET 4: Pobiera dane z cache z sprawdzaniem TTL
        
        Args:
            cache_dict: SÅ‚ownik cache
            key: Klucz cache
            
        Returns:
            dict lub None: Cached data lub None jeÅ›li expired/missing
        """
        if key not in cache_dict:
            return None
            
        cached_item = cache_dict[key]
        current_time = datetime.now().timestamp()
        
        # SprawdÅº TTL
        if current_time - cached_item['timestamp'] > self._cache_ttl_seconds:
            # UsuÅ„ expired entry
            del cache_dict[key]
            logger.info(f"ğŸ—‘ï¸ [CACHE] UsuniÄ™to expired entry: {key}")
            return None
            
        logger.info(f"âœ… [CACHE HIT] Znaleziono w cache: {key}")
        return cached_item['data']

    def _save_to_cache(self, cache_dict: dict, key: str, data: Dict[str, Any]):
        """
        ğŸš€ PRIORYTET 4: Zapisuje dane do cache z TTL i LRU cleanup
        
        Args:
            cache_dict: SÅ‚ownik cache
            key: Klucz cache  
            data: Dane do cache'owania
        """
        # Cleanup jeÅ›li cache za duÅ¼y (prosta LRU - usuÅ„ najstarsze)
        if len(cache_dict) >= self._cache_max_size:
            oldest_key = min(cache_dict.keys(), key=lambda k: cache_dict[k]['timestamp'])
            del cache_dict[oldest_key]
            logger.info(f"ğŸ—‘ï¸ [CACHE LRU] UsuniÄ™to najstarszy entry: {oldest_key}")
        
        # Zapisz nowy entry
        cache_dict[key] = {
            'data': data,
            'timestamp': datetime.now().timestamp()
        }
        logger.info(f"ğŸ’¾ [CACHE SAVE] Zapisano do cache: {key}")

    def _generate_unique_suggestion_ids(self) -> Dict[str, str]:
        """
        Generuje unikalne ID dla sugestii zgodnie z Blueprint Feedback Loop
        """
        return {
            "quick_response_id": f"qr_{uuid.uuid4().hex[:6]}",
            "sq_1_id": f"sq_{uuid.uuid4().hex[:6]}",
            "sq_2_id": f"sq_{uuid.uuid4().hex[:6]}",
            "sq_3_id": f"sq_{uuid.uuid4().hex[:6]}"
        }

    async def generate_analysis(
        self,
        user_input: str,
        client_profile: Dict[str, Any],
        session_history: List[Dict[str, Any]],
        session_context: Optional[Dict[str, Any]] = None,
        mode: str = 'suggestion',
        session_psychology: Optional[Dict[str, Any]] = None,  # DEPRECATED v4.0: UÅ¼ywaj holistic_profile
        holistic_profile: Optional[Dict[str, Any]] = None     # NOWY v4.0: DNA Klienta z Syntezatora
    ) -> Dict[str, Any]:
        """
        ğŸ§  ULTRA MÃ“ZG FAZA 2 - GENERATOR STRATEGII (v4.0)
        
        Drugi "neuron" Ultra MÃ³zgu - generuje precyzyjne wsparcie taktyczne dla sprzedawcy
        na podstawie DNA Klienta (holistyczny profil z Syntezatora).
        
        Args:
            user_input: WejÅ›cie od sprzedawcy (obserwacje, pytania klienta)
            client_profile: Profil klienta (archetyp, tagi, notatki)
            session_history: Historia ostatnich interakcji w sesji
            session_context: Dodatkowy kontekst sesji
            mode: Tryb dziaÅ‚ania ('suggestion' dla sprzedaÅ¼y, 'training' dla AI Dojo)
            session_psychology: DEPRECATED - uÅ¼yj holistic_profile
            holistic_profile: DNA Klienta - holistyczny profil z Syntezatora (gÅ‚Ã³wny input)
            
        Returns:
            SÅ‚ownik z analizÄ… strategicznÄ… opartÄ… na DNA Klienta
            lub odpowiedÅº AI Dojo (training mode)
            
        Raises:
            Exception: Gdy nie udaÅ‚o siÄ™ wygenerowaÄ‡ odpowiedzi
        """
        # === AI DOJO: ROZGAÅÄ˜ZIENIE LOGIKI ===
        if mode == 'training':
            # Tryb treningowy AI Dojo - zupeÅ‚nie oddzielna logika
            return await self._handle_training_conversation(
                user_input=user_input,
                client_profile=client_profile,
                session_history=session_history,
                session_context=session_context
            )
        
        # === ULTRA MÃ“ZG v4.0: DWUETAPOWA ARCHITEKTURA ===
        # SprawdÅº czy mamy holistyczny profil (DNA Klienta) z Syntezatora
        if holistic_profile and not holistic_profile.get('is_fallback'):
            # PRAWDZIWY ULTRA MÃ“ZG: uÅ¼ywamy DNA Klienta
            logger.info("ğŸ§ âš¡ [ULTRA MÃ“ZG] AktywujÄ™ Generator Strategii z DNA Klienta")
            return await self._run_strategic_generator(
                user_input=user_input,
                client_profile=client_profile,
                holistic_profile=holistic_profile,
                session_history=session_history,
                session_context=session_context or {}
            )
        elif holistic_profile and holistic_profile.get('is_fallback'):
            # FALLBACK ULTRA MÃ“ZG: podstawowy profil, dodaj do system prompt
            logger.info("âš ï¸ [ULTRA MÃ“ZG] UÅ¼ywam fallback DNA - standardowa analiza z wskazÃ³wkami")
            # Logika zostanie obsÅ‚uÅ¼ona w _build_system_prompt przez session_psychology fallback
        
        # === ISTNIEJÄ„CA LOGIKA SPRZEDAÅ»OWA (mode='suggestion') ===
        # UWAGA: UÅ¼ywana gdy brak holistycznego profilu lub tryb fallback
        start_time = datetime.now()
        
        try:
            # === POCZÄ„TEK NOWEJ LOGIKI RAG ===
            logger.info("ğŸ” RAG: Rozpoczynam pobieranie wiedzy kontekstowej z Qdrant")
            
            # Krok 1: Pobierz relevantnÄ… wiedzÄ™ z Qdrant
            relevant_knowledge = []
            try:
                client_archetype = client_profile.get("archetype")
                relevant_knowledge = await asyncio.to_thread(
                    self.qdrant_service.search_knowledge,
                    query=user_input,
                    archetype=client_archetype,  # type: ignore
                    limit=3  # Pobieramy 3 najbardziej trafne wyniki
                )
                logger.info(f"âœ… RAG: Znaleziono {len(relevant_knowledge)} relevantnych wskazÃ³wek")
                
                # Loguj trafnoÅ›Ä‡ wynikÃ³w
                for i, nugget in enumerate(relevant_knowledge):
                    score = nugget.get('score', 0)
                    title = nugget.get('title', 'Bez tytuÅ‚u')[:50]
                    logger.debug(f"  #{i+1}: {title}... (score: {score:.3f})")
                    
            except Exception as e:
                # W razie bÅ‚Ä™du Qdrant, kontynuujemy bez dodatkowej wiedzy
                logger.warning(f"âš ï¸ RAG: BÅ‚Ä…d podczas wyszukiwania w Qdrant: {e}")
                relevant_knowledge = []

            # Krok 2: Sformatuj pobranÄ… wiedzÄ™ do czytelnej formy dla LLM
            knowledge_context = "BRAK DODATKOWEGO KONTEKSTU Z BAZY WIEDZY."
            if relevant_knowledge:
                formatted_nuggets = []
                for i, nugget in enumerate(relevant_knowledge):
                    title = nugget.get("title", "Brak tytuÅ‚u")
                    content = nugget.get("content", "Brak treÅ›ci")
                    score = nugget.get("score", 0)
                    knowledge_type = nugget.get("knowledge_type", "general")
                    
                    formatted_nuggets.append(
                        f"{i+1}. [{knowledge_type.upper()}] {title}\n"
                        f"   TreÅ›Ä‡: {content}\n"
                        f"   TrafnoÅ›Ä‡: {score:.1%}"
                    )
                
                knowledge_context = "\n---\n".join(formatted_nuggets)
                logger.info(f"ğŸ“š RAG: Kontekst wiedzy przygotowany ({len(knowledge_context)} znakÃ³w)")
                
            # === KONIEC NOWEJ LOGIKI RAG ===

            # Wygeneruj unikalne ID dla sugestii (Blueprint Feedback Loop)
            suggestion_ids = self._generate_unique_suggestion_ids()
            
            # Krok 3: Zbuduj wzbogacony prompt systemowy (z wiedzÄ… z RAG + Psychology)
            system_prompt = self._build_system_prompt(
                client_profile=client_profile,
                session_history=session_history,
                session_context=session_context or {},
                knowledge_context=knowledge_context,  # NOWY PARAMETR
                suggestion_ids=suggestion_ids,  # ID dla granularnego feedback
                session_psychology=session_psychology  # NOWY v4.0: Psychology data
            )
            
            # Zbuduj prompt uÅ¼ytkownika
            user_prompt = self._build_user_prompt(user_input)
            
            logger.info(f"ğŸ¤– AI Service: WysyÅ‚anie zapytania do modelu {self.model_name}")
            logger.debug(f"User input: '{user_input[:100]}...'")
            
            # WywoÅ‚aj model LLM z retry logic
            response = await self._call_llm_with_retry(
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
            
            # Parsuj odpowiedÅº JSON
            analysis = self._parse_llm_response(response)
            
            # Oblicz czas przetwarzania
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            
            # Dodaj metadata
            analysis["processing_time_ms"] = int(processing_time)
            analysis["model_used"] = self.model_name
            analysis["timestamp"] = datetime.now().isoformat()
            
            logger.info(f"âœ… AI Service: Analiza wygenerowana w {processing_time:.0f}ms")
            logger.debug(f"Confidence level: {analysis.get('confidence_level', 'N/A')}")
            
            return analysis
            
        except Exception as e:
            logger.error(f"âŒ AI Service: BÅ‚Ä…d podczas generowania analizy: {e}")
            
            # ZwrÃ³Ä‡ fallback response
            return self._create_fallback_response(user_input, str(e))
    
    def _build_system_prompt(
        self,
        client_profile: Dict[str, Any],
        session_history: List[Dict[str, Any]],
        session_context: Dict[str, Any],
        knowledge_context: str = "BRAK DODATKOWEGO KONTEKSTU Z BAZY WIEDZY.",
        suggestion_ids: Optional[Dict[str, str]] = None,
        session_psychology: Optional[Dict[str, Any]] = None  # NOWY v4.0: Psychology data
    ) -> str:
        """
        Zbuduj dynamiczny prompt systemowy dla LLM - NOWA WERSJA PRO-TESLA
        """
        
        # === WARSTWA 1: NADRZÄ˜DNA MISJA (NON-NEGOTIABLE) ===
        system_prompt = """JesteÅ› elitarnym Co-Pilotem SprzedaÅ¼y TESLI. Twoim jedynym i absolutnym celem jest maksymalizacja sprzedaÅ¼y samochodÃ³w MARKI TESLA. Twoja lojalnoÅ›Ä‡ wobec marki Tesla jest absolutna. Nigdy, pod Å¼adnym pozorem, nie proponujesz produktÃ³w konkurencji, nie chwalisz ich, ani nie zachÄ™casz do ich testowania. JesteÅ› stronniczym, ale niezwykle skutecznym ekspertem Tesli.

"""

        # === WARSTWA 2: DEFINICJA WROGA (KONKURENCJA) ===
        system_prompt += """GÅ‚Ã³wni konkurenci, o ktÃ³rych moÅ¼e wspomnieÄ‡ klient, to Audi (szczegÃ³lnie Q4 e-tron), BMW (szczegÃ³lnie iX3), Mercedes, BYD. Twoim zadaniem jest zidentyfikowanie, kiedy klient o nich wspomina, i natychmiastowe oraz skuteczne przekierowanie rozmowy na unikalne przewagi i korzyÅ›ci Tesli.

"""

        # === WARSTWA 3: ZÅOTE ZASADY (HARD RULES) ===
        system_prompt += """TWOJE ZASADY SÄ„ NIEZÅOMNE:
1.  **NIGDY NIE REKOMENDUJ PRODUKTU KONKURENCJI.** Nigdy nie sugeruj jazdy prÃ³bnej, nie analizuj pozytywnie ich cech, nie chwal ich designu. To jest zdrada Twojej misji.
2.  **ZAWSZE PRZEKIEROWUJ ROZMOWÄ˜.** Gdy klient chwali cechÄ™ konkurenta (np. "design Audi"), Twoja odpowiedÅº musi byÄ‡ w stylu: "CieszÄ™ siÄ™, Å¼e docenia Pan nowoczesny styl. W Tesli podchodzimy do designu holistycznie, Å‚Ä…czÄ…c go z aerodynamikÄ… i oprogramowaniem, co daje unikalne korzyÅ›ci, takie jak...". Zawsze wracaj do Tesli.
3.  **TWOIM PRODUKTEM JEST TESLA.** Koniec. Twoje sugestie, pytania i analizy muszÄ… w efekcie prowadziÄ‡ do zwiÄ™kszenia zainteresowania i chÄ™ci zakupu Tesli.
4.  **BÄ„DÅ¹ SUBTELNY, ALE ZDECYDOWANY.** Nie krytykuj bezpoÅ›rednio konkurencji. Zamiast tego, podkreÅ›laj obszary, w ktÃ³rych Tesla ma obiektywnÄ… i druzgocÄ…cÄ… przewagÄ™ (sieÄ‡ Supercharger, oprogramowanie i aktualizacje OTA, TCO, bezpieczeÅ„stwo, ekosystem).

"""

        # === WARSTWA 4: OSOBOWOÅšÄ† I TON ===
        system_prompt += """Twoja osobowoÅ›Ä‡ to poÅ‚Ä…czenie Elona Muska (wizjonerstwo, odwaga) i Steve'a Jobsa (obsesja na punkcie produktu i doÅ›wiadczenia uÅ¼ytkownika). JesteÅ› pasjonatem, ekspertem i strategiem. TwÃ³j ton jest pewny siebie, profesjonalny i inspirujÄ…cy.

"""

        # === WARSTWA 5: KONTEKST WIEDZY Z BAZY RAG (KLUCZOWE!) ===
        system_prompt += f"""
=== SPECJALISTYCZNA WIEDZA Z BAZY DANYCH ===
Na podstawie bieÅ¼Ä…cej sytuacji, oto najwaÅ¼niejsze informacje z naszej bazy wiedzy, ktÃ³re MUSISZ uwzglÄ™dniÄ‡ w swojej analizie i rekomendacjach:

{knowledge_context}

INSTRUKCJE DOTYCZÄ„CE WIEDZY:
- Wykorzystaj powyÅ¼sze informacje do stworzenia precyzyjnych, merytorycznie uzasadnionych odpowiedzi
- JeÅ›li wiedza zawiera konkretne dane (np. limity podatkowe, programy dopÅ‚at), uÅ¼yj ich w swoich argumentach
- OdwoÅ‚uj siÄ™ do tych informacji naturalnie, nie wspominajÄ…c Å¼e pochodzÄ… z "bazy danych"
- Traktuj tÄ™ wiedzÄ™ jako swoje wÅ‚asne, eksperckie kompetencje

"""
        
        # === WARSTWA 6: ULTRA MÃ“ZG - DNA KLIENTA (v4.0) ===
        # GENERATOR STRATEGII - logika przeniesiona do gÅ‚Ã³wnej funkcji generate_analysis
        # Ta warstwa obecnie nieuÅ¼ywana - logic w generate_analysis
        pass
        
        # === WARSTWA 7: KONTEKST ROZMOWY (Dynamiczna czÄ™Å›Ä‡) ===
        # Dodaj profil klienta
        if client_profile:
            system_prompt += f"""
PROFIL KLIENTA:
- Alias: {client_profile.get('alias', 'Nieznany')}
- Archetyp: {client_profile.get('archetype', 'Nieznany')} 
- Tagi profilujÄ…ce: {', '.join(client_profile.get('tags', []))}
- Notatki analityczne: {client_profile.get('notes', 'Brak notatek')}

"""
        
        # Dodaj kontekst sesji
        if session_context:
            session_type = session_context.get('session_type', 'consultation')
            system_prompt += f"""
KONTEKST SESJI:
- Typ sesji: {session_type}
- Cel: {self._get_session_goal(session_type)}

"""
        
        # Dodaj historiÄ™ interakcji
        if session_history:
            system_prompt += """
HISTORIA SESJI (ostatnie interakcje):
"""
            for i, interaction in enumerate(session_history[-3:], 1):  # Ostatnie 3 interakcje
                timestamp = interaction.get('timestamp', 'nieznany czas')
                user_input = interaction.get('user_input', '')
                confidence = interaction.get('confidence_score', 'N/A')
                
                system_prompt += f"""
{i}. [{timestamp}] Sprzedawca: "{user_input[:200]}..."
   (Poprzednia pewnoÅ›Ä‡ AI: {confidence}%)

"""
        
        # === WARSTWA 7: NARZÄ˜DZIA ANALITYCZNE I FORMAT WYJÅšCIOWY ===
        # Instrukcje wyjÅ›ciowe z nowymi zasadami  
        system_prompt += """
TWOJE NARZÄ˜DZIA ANALITYCZNE (Frameworki):
- Psychologia sprzedaÅ¼y Tesla (archetypy klientÃ³w)
- Analiza sygnaÅ‚Ã³w kupna i oporu
- Strategiczne zarzÄ…dzanie zastrzeÅ¼eniami  
- Budowanie wartoÅ›ci emocjonalnej i logicznej
- Timing i pilnoÅ›Ä‡ w procesie decyzyjnym

KLUCZOWE ZASADY GENEROWANIA ODPOWIEDZI:
1. Quick Response (OdpowiedÅº Holistyczna): GenerujÄ…c pole quick_response, przeanalizuj CAÅÄ„ dotychczasowÄ… historiÄ™ rozmowy. OdpowiedÅº musi byÄ‡ krÃ³tka, naturalna i spÃ³jna z caÅ‚ym znanym kontekstem.
2. Pytania PogÅ‚Ä™biajÄ…ce (OdpowiedÅº Atomowa): GenerujÄ…c listÄ™ suggested_questions, skup siÄ™ WYÅÄ„CZNIE na ostatniej, bieÅ¼Ä…cej wypowiedzi/obserwacji uÅ¼ytkownika. Pytania muszÄ… dotyczyÄ‡ tego konkretnego punktu i pomagaÄ‡ go zgÅ‚Ä™biÄ‡.

WYMAGANY FORMAT ODPOWIEDZI:
ZwrÃ³Ä‡ WYÅÄ„CZNIE poprawny JSON zgodny z tym schematem (bez dodatkowego tekstu):

{
    "quick_response": {
        "id": "{quick_response_id}",
        "text": "KrÃ³tka, naturalna odpowiedÅº spÃ³jna z CAÅÄ„ historiÄ… rozmowy - gotowa do natychmiastowego uÅ¼ycia"
    },
    
    "suggested_questions": [
        {
            "id": "{sq_1_id}",
            "text": "Pytanie pogÅ‚Ä™biajÄ…ce dotyczÄ…ce TYLKO ostatniej wypowiedzi klienta?"
        },
        {
            "id": "{sq_2_id}",
            "text": "Drugie pytanie o ten sam konkretny punkt?"
        }
    ],
    
    "main_analysis": "Holistyczna analiza caÅ‚ej sytuacji na podstawie peÅ‚nej historii (2-3 zdania)",
    "client_archetype": "Zidentyfikowany archetyp na podstawie caÅ‚oksztaÅ‚tu interakcji",
    "confidence_level": 85,
    
    "likely_archetypes": [
        {"name": "DominujÄ…cy Archetyp", "confidence": 85, "description": "KrÃ³tki opis"},
        {"name": "Drugi Archetyp", "confidence": 45, "description": "KrÃ³tki opis"}
    ],
    
    "strategic_notes": [
        "Kluczowy insight strategiczny nr 1",
        "Kluczowy insight strategiczny nr 2", 
        "Kluczowy insight strategiczny nr 3"
    ],
    
    "suggested_actions": [
        {"action": "Konkretna akcja 1", "reasoning": "Dlaczego ta akcja"},
        {"action": "Konkretna akcja 2", "reasoning": "Dlaczego ta akcja"},
        {"action": "Konkretna akcja 3", "reasoning": "Dlaczego ta akcja"}
    ],
    
    "buy_signals": ["sygnaÅ‚ zakupu 1", "sygnaÅ‚ zakupu 2"],
    "risk_signals": ["sygnaÅ‚ ryzyka 1", "sygnaÅ‚ ryzyka 2"],
    
    "objection_handlers": {
        "potencjalny zarzut 1": "sposÃ³b odpowiedzi na zarzut",
        "potencjalny zarzut 2": "sposÃ³b odpowiedzi na zarzut"
    },
    
    "sentiment_score": 7,
    "potential_score": 6,
    "urgency_level": "medium",
    
    "next_best_action": "NajwaÅ¼niejsza nastÄ™pna akcja wynikajÄ…ca z caÅ‚oksztaÅ‚tu analizy",
    "follow_up_timing": "Rekomendowany timing nastÄ™pnego kontaktu"
}

KRYTYCZNE INSTRUKCJE WYKONANIA:
1. QUICK RESPONSE = Holistyczna: UwzglÄ™dnij caÅ‚Ä… historiÄ™, kontekst klienta, jego archetyp i wszystkie poprzednie interakcje
2. SUGGESTED QUESTIONS = Atomowa: Ignoruj historiÄ™, skup siÄ™ tylko na ostatniej wypowiedzi i jak jÄ… gÅ‚Ä™biej zbadaÄ‡
3. Pole "likely_archetypes" musi zawieraÄ‡ 1-2 najbardziej prawdopodobne archetypy z procentowym dopasowaniem
4. Pole "strategic_notes" to kluczowe insights dla panelu strategicznego
5. Quick response ma byÄ‡ gotowy do natychmiastowego wypowiedzenia - maksymalnie 2 zdania

PAMIÄ˜TAJ: Odpowiadaj TYLKO w JSON. Å»adnego dodatkowego tekstu przed ani po JSON!
"""
        
        # ZastÄ…p placeholdery ID prawdziwymi wartoÅ›ciami (Blueprint Feedback Loop)
        if suggestion_ids:
            # Prostsze podejÅ›cie - bezpoÅ›rednie zastÄ…pienie bez .format()
            system_prompt = system_prompt.replace("{quick_response_id}", suggestion_ids["quick_response_id"])
            system_prompt = system_prompt.replace("{sq_1_id}", suggestion_ids["sq_1_id"])
            system_prompt = system_prompt.replace("{sq_2_id}", suggestion_ids["sq_2_id"])
        
        return system_prompt
    
    def _build_user_prompt(self, user_input: str) -> str:
        """
        Zbuduj prompt uÅ¼ytkownika na podstawie jego wejÅ›cia
        """
        return f"""
AKTUALNA SYTUACJA:
Sprzedawca raportuje: "{user_input}"

Przeanalizuj tÄ™ sytuacjÄ™ i dostarcz inteligentnych rekomendacji w formacie JSON.
"""
    
    def _get_session_goal(self, session_type: str) -> str:
        """
        ZwrÃ³Ä‡ cel sesji na podstawie jej typu
        """
        goals = {
            "consultation": "Zrozumienie potrzeb klienta i zbudowanie zaangaÅ¼owania",
            "follow-up": "Kontynuacja rozmowy i przesuniÄ™cie w stronÄ™ decyzji",
            "negotiation": "Negocjacja warunkÃ³w i zamkniÄ™cie transakcji", 
            "demo": "Prezentacja produktu i wzbudzenie emocji",
            "closing": "Finalizacja sprzedaÅ¼y i podpisanie umowy"
        }
        return goals.get(session_type, "OgÃ³lne wsparcie sprzedaÅ¼owe")
    
    async def _call_llm_with_retry(
        self,
        system_prompt: str,
        user_prompt: str
    ) -> str:
        """
        WywoÅ‚aj model LLM z logikÄ… retry
        """
        last_exception = None
        
        for attempt in range(self.max_retries):
            try:
                logger.debug(f"ğŸ”„ AI Service: PrÃ³ba {attempt + 1}/{self.max_retries}")
                
                # WywoÅ‚anie Ollama w asynchronicznym kontekÅ›cie
                response = await asyncio.to_thread(
                    self._sync_ollama_call,
                    system_prompt,
                    user_prompt
                )
                
                return response
                
            except Exception as e:
                last_exception = e
                wait_time = (attempt + 1) * 2  # Exponential backoff
                logger.warning(f"âš ï¸ AI Service: PrÃ³ba {attempt + 1} nieudana: {e}")
                
                if attempt < self.max_retries - 1:
                    logger.info(f"â³ Ponawiam za {wait_time} sekund...")
                    await asyncio.sleep(wait_time)
        
        # Wszystkie prÃ³by nieudane
        raise Exception(f"LLM call failed after {self.max_retries} attempts. Last error: {last_exception}")
    
    def _sync_ollama_call(self, system_prompt: str, user_prompt: str) -> str:
        """
        Synchroniczne wywoÅ‚anie Ollama Cloud (uruchomione w thread)
        """
        if self.client is None:
            raise ConnectionError("Klient Ollama nie zostaÅ‚ poprawnie zainicjalizowany.")

        response = self.client.chat(
            model=self.model_name,  # UÅ¼ywamy settings.OLLAMA_MODEL (gpt-oss:120b)
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': user_prompt}
            ]
        )
        
        # Ollama moÅ¼e zwrÃ³ciÄ‡ rÃ³Å¼ne struktury odpowiedzi
        if isinstance(response, dict):
            if 'message' in response and 'content' in response['message']:
                return response['message']['content']
            elif 'content' in response:
                return response['content']
        elif isinstance(response, str):
            return response
            
        # Fallback - sprÃ³buj przekonwertowaÄ‡ na string
        return str(response)
    
    def _parse_llm_response(self, llm_response: str) -> Dict[str, Any]:
        """
        Parsuj odpowiedÅº LLM do struktury JSON
        """
        try:
            # UsuÅ„ potencjalne biaÅ‚e znaki i znajdÅº JSON
            cleaned_response = llm_response.strip()
            
            # ZnajdÅº poczÄ…tek i koniec JSON (na wypadek dodatkowego tekstu)
            start_idx = cleaned_response.find('{')
            end_idx = cleaned_response.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                raise ValueError("Nie znaleziono JSON w odpowiedzi LLM")
            
            json_str = cleaned_response[start_idx:end_idx]
            
            # Parsuj JSON
            parsed_data = json.loads(json_str)
            
            # Waliduj przez Pydantic schema
            interaction_response = InteractionResponse(**parsed_data)
            
            # ZwrÃ³Ä‡ jako dict
            return interaction_response.model_dump()
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ JSON parsing error: {e}")
            logger.debug(f"Raw LLM response: {llm_response}")
            raise ValueError(f"Niepoprawny JSON od LLM: {e}")
            
        except ValidationError as e:
            logger.error(f"âŒ Pydantic validation error: {e}")  
            logger.debug(f"Parsed data: {parsed_data}")
            raise ValueError(f"OdpowiedÅº LLM nie pasuje do schematu: {e}")
    
    def _create_fallback_response(self, user_input: str, error_msg: str) -> Dict[str, Any]:
        """
        StwÃ³rz fallback response gdy LLM nie dziaÅ‚a (z unikalnymi ID dla Feedback Loop)
        """
        logger.warning(f"ğŸ”„ AI Service: UÅ¼ywam fallback response dla: '{user_input[:50]}...'")
        
        # Wygeneruj unikalne ID dla fallback sugestii
        fallback_ids = self._generate_unique_suggestion_ids()
        
        return {
            "main_analysis": f"Analiza Tesla Co-Pilot: '{user_input[:100]}...' - PoÅ‚Ä…czenie z AI chwilowo niedostÄ™pne. Skup siÄ™ na unikatowych przewagach Tesli: Supercharger, OTA updates, bezpieczeÅ„stwo 5-gwiazdek.",
            "client_archetype": "Nieznany (bÅ‚Ä…d AI)",
            "confidence_level": 30,
            
            "suggested_actions": [
                {"action": "PodkreÅ›l przewagi sieci Supercharger", "reasoning": "Unikalna przewaga Tesli nad konkurencjÄ…"},
                {"action": "OmÃ³w aktualizacje OTA", "reasoning": "Auto ktÃ³re ciÄ…gle siÄ™ rozwija - tego nie ma konkurencja"},
                {"action": "Zaprezentuj najwyÅ¼sze oceny bezpieczeÅ„stwa", "reasoning": "Tesla liderem w testach NHTSA i Euro NCAP"},
                {"action": "PokaÅ¼ oszczÄ™dnoÅ›ci TCO", "reasoning": "DÅ‚ugoterminowa wartoÅ›Ä‡ przewyÅ¼sza konkurencjÄ™"}
            ],
            
            "buy_signals": ["pytania o Tesli", "zainteresowanie technologiÄ…"],
            "risk_signals": ["porÃ³wnania z konkurencjÄ…", "wahania cenowe"],
            
            "key_insights": [
                "AI niedostÄ™pny - skup siÄ™ na przewagach Tesli",
                "Tesla = jedyna marka z prawdziwÄ… autonomiÄ…",
                "Supercharger network to game changer"
            ],
            
            "objection_handlers": {
                "za drogo": "PokaÅ¼ oszczÄ™dnoÅ›ci paliwowe i serwisowe - Tesla ma najniÅ¼sze TCO",
                "potrzebujÄ™ czasu": "Zapytaj o obecny samochÃ³d i koszty eksploatacji",
                "konkurencja taÅ„sza": "PorÃ³wnaj peÅ‚ny ekosystem: zasiÄ™g, Å‚adowanie, oprogramowanie"
            },
            
            "qualifying_questions": [
                "Jak daleko Pan zwykle jeÅºdzi dziennie? (Tesla ma najlepszy zasiÄ™g)",
                "Czy korzysta Pan z szybkich tras miÄ™dzymiastowych? (Supercharger advantage)",
                "Co myÅ›li Pan o samochodach, ktÃ³re same siÄ™ aktualizujÄ… jak smartfony?"
            ],
            
            "sentiment_score": 5,
            "potential_score": 5,
            "urgency_level": "medium",
            
            "next_best_action": "Zbierz wiÄ™cej informacji o potrzebach klienta",
            "follow_up_timing": "W ciÄ…gu 24-48 godzin",
            
            # Natychmiastowa odpowiedÅº (fallback z unikalnym ID)
            "quick_response": {
                "id": fallback_ids["quick_response_id"],
                "text": "Rozumiem. Czy mÃ³gÅ‚by Pan powiedzieÄ‡ wiÄ™cej o swoich potrzebach? Tesla oferuje rozwiÄ…zania dla kaÅ¼dego stylu Å¼ycia."
            },
            
            # Sugerowane pytania (fallback z unikalnymi ID)
            "suggested_questions": [
                {
                    "id": fallback_ids["sq_1_id"],
                    "text": "Jakie sÄ… Pana gÅ‚Ã³wne priorytety przy wyborze samochodu?"
                },
                {
                    "id": fallback_ids["sq_2_id"],
                    "text": "Czy rozwaÅ¼aÅ‚ Pan wczeÅ›niej samochÃ³d elektryczny?"
                }
            ],
            
            # Metadata bÅ‚Ä™du
            "is_fallback": True,
            "error_reason": error_msg,
            "processing_time_ms": 0,
            "model_used": f"{self.model_name} (fallback)",
            "timestamp": datetime.now().isoformat()
        }

    # === AI DOJO: NOWE FUNKCJE TRENINGOWE ===
    
    async def _handle_training_conversation(
        self,
        user_input: str,
        client_profile: Dict[str, Any],
        session_history: List[Dict[str, Any]],
        session_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ObsÅ‚uga konwersacji treningowej w AI Dojo
        
        Tryb 'training': AI dziaÅ‚a jak analityk wiedzy, zadaje pytania doprecyzowujÄ…ce,
        strukturyzuje informacje i przygotowuje je do zapisu w bazie Qdrant
        
        Args:
            user_input: WiadomoÅ›Ä‡ od eksperta/administratora
            client_profile: Profil klienta (jeÅ›li trening dotyczy konkretnego przypadku)
            session_history: Historia konwersacji treningowej
            session_context: Kontekst treningu
            
        Returns:
            Dict zgodny z DojoMessageResponse (response, response_type, structured_data, etc.)
        """
        start_time = datetime.now()
        
        try:
            logger.info("ğŸ“ AI Dojo: Rozpoczynam przetwarzanie wiadomoÅ›ci treningowej")
            logger.debug(f"Training input: '{user_input[:100]}...'")
            
            # Zbuduj specjalny prompt dla trybu treningowego
            training_prompt = self._build_training_system_prompt(
                client_profile=client_profile,
                session_history=session_history,
                session_context=session_context or {}
            )
            
            # Zbuduj user prompt dla treningu
            user_prompt = self._build_training_user_prompt(user_input)
            
            logger.info(f"ğŸ¤– AI Dojo: WysyÅ‚anie do modelu {self.model_name} (tryb treningowy)")
            
            # WywoÅ‚aj LLM z retry logic (uÅ¼ywamy tej samej funkcji co w trybie sprzedaÅ¼owym)
            response = await self._call_llm_with_retry(
                system_prompt=training_prompt,
                user_prompt=user_prompt
            )
            
            # Parsuj odpowiedÅº AI Dojo (inna logika niÅ¼ w trybie sprzedaÅ¼owym)
            training_analysis = self._parse_training_response(response)
            
            # Oblicz czas przetwarzania
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            
            # Dodaj metadata
            training_analysis["processing_time_ms"] = int(processing_time)
            training_analysis["timestamp"] = datetime.now().isoformat()
            training_analysis["model_used"] = f"{self.model_name} (training mode)"
            
            logger.info(f"âœ… AI Dojo: Analiza wygenerowana w {processing_time:.0f}ms")
            logger.debug(f"Response type: {training_analysis.get('response_type', 'unknown')}")
            
            return training_analysis
            
        except Exception as e:
            logger.error(f"âŒ AI Dojo: BÅ‚Ä…d podczas przetwarzania: {e}")
            
            # Fallback response dla AI Dojo
            return self._create_training_fallback_response(user_input, str(e))
    
    def _build_training_system_prompt(
        self,
        client_profile: Dict[str, Any],
        session_history: List[Dict[str, Any]],
        session_context: Dict[str, Any]
    ) -> str:
        """
        Zbuduj prompt systemowy dla AI Dojo (tryb treningowy)
        
        UWAGA: To jest zupeÅ‚nie inny prompt niÅ¼ w trybie sprzedaÅ¼owym!
        """
        
        system_prompt = """JesteÅ› EKSPERTEM STRUKTURYZACJI WIEDZY dla systemu sprzedaÅ¼y Tesla Co-Pilot AI.

=== TWOJA MISJA ===
Otrzymujesz informacje od ekspertÃ³w sprzedaÅ¼y i SZYBKO przeksztaÅ‚casz je w uÅ¼ytecznÄ…, strukturalnÄ… wiedzÄ™ dla systemu. JesteÅ› PROAKTYWNY i EFEKTYWNY.

=== ZÅOTE ZASADY (PRIORITY ORDER) ===

1. **NAJPIERW: SprawdÅº czy masz WYSTARCZAJÄ„CE informacje do przygotowania wiedzy**
   - JeÅ›li tak â†’ NATYCHMIAST przygotuj structured_data (response_type: "confirmation")
   - JeÅ›li nie â†’ zadaj MAKSYMALNIE 1-2 konkretne pytania (response_type: "question")

2. **MINIMALIZUJ PYTANIA**: Nie zadawaj wiÄ™cej niÅ¼ 2 pytaÅ„. Po 2 pytaniach ZAWSZE przygotuj dane na podstawie tego co masz.

3. **AKCJA nad PERFEKCJÄ„**: Lepiej przygotowaÄ‡ niekompletnÄ… wiedzÄ™ niÅ¼ pytaÄ‡ w nieskoÅ„czonoÅ›Ä‡.

=== KONTEKST TESLA & AUTOMATYCZNE UZUPEÅNIANIE ===

Gdy przygotowujesz wiedzÄ™ o sprzedaÅ¼y Tesla, automatycznie uzupeÅ‚nij braki:

**Dla pytaÅ„ o CENÄ˜ Tesla (przykÅ‚ad z user input):**
- Typ wiedzy: "objection" lub "pricing"
- Archetyp: null (uniwersalne, chyba Å¼e user wskaÅ¼e konkretny)
- Tagi: ["cena", "finansowanie", "wartoÅ›Ä‡", "roi"]
- TreÅ›Ä‡: Skoncentruj siÄ™ na TCO, oszczÄ™dnoÅ›ciach, leasing, porÃ³wnaniu z kosztami benzyny

**Dla pytaÅ„ technicznych:**
- Typ wiedzy: "technical" lub "product"
- Tagi: ["specyfikacja", "porÃ³wnanie", "funkcje"]

**Dla obsÅ‚ugi zastrzeÅ¼eÅ„:**
- Typ wiedzy: "objection"
- Tagi: ["obiekcja", "odpowiedÅº", "persuasion"]

=== SMART DEFAULTS ===
JeÅ›li user nie podaÅ‚ wszystkich szczegÃ³Å‚Ã³w, uÅ¼yj inteligentnych domyÅ›lnych wartoÅ›ci:
- knowledge_type: Dedukuj z kontekstu (cena=pricing, zastrzeÅ¼enia=objection, funkcje=product)
- archetype: null (uniwersalne) chyba Å¼e jasno wskazano
- source: "Ekspert sprzedaÅ¼y" lub nazwa z kontekstu
- tags: Automatycznie wygeneruj 3-5 relevantnych tagÃ³w

=== PRZYKÅADY NATYCHMIASTOWEGO STRUKTURYZOWANIA ===

**INPUT: "Jak najlepiej odpowiadaÄ‡ klientom pytajÄ…cym o cenÄ™ Tesla?"**
â†’ NATYCHMIAST przygotuj structured_data z typem "objection", tagami ["cena", "finansowanie", "tco"] 

**INPUT: "Tesla Model Y ma nowÄ… opcjÄ™ kolorystycznÄ…"**
â†’ NATYCHMIAST przygotuj structured_data z typem "product", tagami ["model-y", "kolory", "opcje"]

**INPUT: "Klient mÃ³wi Å¼e zasiÄ™g to za maÅ‚o"**
â†’ NATYCHMIAST przygotuj structured_data z typem "objection", tagami ["zasiÄ™g", "obiekcje", "range-anxiety"]

**TYLKO zadawaj pytania gdy:**
- Informacja jest bardzo ogÃ³lna ("pomoc", "problem")
- Brakuje kluczowych danych technicznych (konkretne liczby, modele)
- User pyta o coÅ› co nie dotyczy sprzedaÅ¼y Tesla

"""
        
        # Dodaj kontekst sesji treningowej
        if session_context:
            training_mode = session_context.get('training_mode', 'knowledge_update')
            system_prompt += f"""
=== TRYB TRENINGU ===
Aktualny tryb: {training_mode}
"""
            
        # Dodaj historiÄ™ konwersacji treningowej
        if session_history:
            system_prompt += """
=== HISTORIA KONWERSACJI TRENINGOWEJ ===
"""
            for i, msg in enumerate(session_history[-5:], 1):  # Ostatnie 5 wiadomoÅ›ci
                timestamp = msg.get('timestamp', 'nieznany czas')
                content = msg.get('message', msg.get('user_input', ''))
                system_prompt += f"""
{i}. [{timestamp}] {content[:200]}...
"""
        
        # Instrukcje wyjÅ›ciowe
        system_prompt += """
=== FORMAT ODPOWIEDZI ===
Odpowiadaj WYÅÄ„CZNIE w formacie JSON zgodnym z jednym z poniÅ¼szych wzorcÃ³w:

**TRYB PYTAÅƒ (gdy potrzebujesz wiÄ™cej informacji):**
{
    "response": "Pytania doprecyzowujÄ…ce lub proÅ›ba o wiÄ™cej szczegÃ³Å‚Ã³w",
    "response_type": "question",
    "confidence_level": 60,
    "suggested_follow_up": ["Pytanie 1?", "Pytanie 2?"]
}

**TRYB STRUKTURYZOWANIA (gdy przygotowujesz dane do zapisu):**
{
    "response": "PrzygotowaÅ‚em kompleksowÄ… wiedzÄ™ o odpowiadaniu na pytania o cenÄ™ Tesla. Czy zapisaÄ‡ w bazie?",
    "response_type": "confirmation", 
    "structured_data": {
        "title": "Skuteczne odpowiedzi na pytania o cenÄ™ Tesla",
        "content": "Strategia odpowiedzi na pytania cenowe: 1) Przekieruj na wartoÅ›Ä‡ (TCO, oszczÄ™dnoÅ›ci paliwowe, serwis), 2) PokaÅ¼ kalkulator porÃ³wnawczy z benzynÄ…, 3) Zaproponuj opcje finansowania (leasing, kredyt), 4) PodkreÅ›l unikalne korzyÅ›ci (Supercharger, aktualizacje OTA, bezpieczeÅ„stwo), 5) Dostosuj do archetypu klienta.",
        "knowledge_type": "objection",
        "archetype": null,
        "tags": ["cena", "finansowanie", "tco", "wartoÅ›Ä‡", "obiekcje"],
        "source": "Administrator"
    },
    "confidence_level": 90
}

**TRYB STATUS (potwierdzenia, bÅ‚Ä™dy):**
{
    "response": "Informacja zostaÅ‚a zapisana/wystÄ…piÅ‚ bÅ‚Ä…d/inne",
    "response_type": "status",
    "confidence_level": 95
}

PAMIÄ˜TAJ: Odpowiadaj TYLKO w JSON, bez dodatkowego tekstu!
"""
        
        return system_prompt
    
    def _build_training_user_prompt(self, user_input: str) -> str:
        """
        Zbuduj prompt uÅ¼ytkownika dla trybu treningowego
        """
        return f"""
WIADOMOÅšÄ† OD EKSPERTA:
"{user_input}"

Przeanalizuj tÄ™ informacjÄ™ i odpowiedz zgodnie z instrukcjami systemowymi.
"""
    
    def _parse_training_response(self, llm_response: str) -> Dict[str, Any]:
        """
        Parsuj odpowiedÅº LLM w trybie treningowym
        
        Inna logika niÅ¼ w trybie sprzedaÅ¼owym - oczekujemy DojoMessageResponse format
        """
        try:
            # UsuÅ„ potencjalne biaÅ‚e znaki i znajdÅº JSON
            cleaned_response = llm_response.strip()
            
            # ZnajdÅº poczÄ…tek i koniec JSON
            start_idx = cleaned_response.find('{')
            end_idx = cleaned_response.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                raise ValueError("Nie znaleziono JSON w odpowiedzi AI Dojo")
            
            json_str = cleaned_response[start_idx:end_idx]
            
            # Parsuj JSON
            parsed_data = json.loads(json_str)
            
            # Walidacja - sprawdÅº czy ma wymagane pola
            required_fields = ["response", "response_type"]
            for field in required_fields:
                if field not in parsed_data:
                    raise ValueError(f"Brakuje wymaganego pola: {field}")
            
            # Dodaj domyÅ›lne wartoÅ›ci jeÅ›li brakuje
            if "confidence_level" not in parsed_data:
                parsed_data["confidence_level"] = 70
            
            logger.debug(f"AI Dojo response type: {parsed_data['response_type']}")
            
            return parsed_data
            
        except json.JSONDecodeError as e:
            logger.error(f"âŒ AI Dojo JSON parsing error: {e}")
            logger.debug(f"Raw AI response: {llm_response}")
            raise ValueError(f"Niepoprawny JSON od AI Dojo: {e}")
            
        except Exception as e:
            logger.error(f"âŒ AI Dojo response parsing error: {e}")
            raise ValueError(f"BÅ‚Ä…d parsowania odpowiedzi AI Dojo: {e}")
    
    def _create_training_fallback_response(self, user_input: str, error_msg: str) -> Dict[str, Any]:
        """
        StwÃ³rz fallback response dla AI Dojo gdy LLM nie dziaÅ‚a
        """
        logger.warning(f"ğŸ”„ AI Dojo: UÅ¼ywam fallback response dla: '{user_input[:50]}...'")
        
        return {
            "response": f"Przepraszam, chwilowo nie mogÄ™ przetworzyÄ‡ Twojej wiadomoÅ›ci: '{user_input[:100]}...'. SprÃ³buj ponownie za chwilÄ™ lub sformuÅ‚uj pytanie inaczej.",
            "response_type": "error",
            "confidence_level": 0,
            "suggested_follow_up": [
                "Czy moÅ¼esz przeformuÅ‚owaÄ‡ swojÄ… wiadomoÅ›Ä‡?",
                "Czy chcesz sprÃ³bowaÄ‡ za chwilÄ™?"
            ],
            "processing_time_ms": 0,
            "model_used": f"{self.model_name} (fallback)",
            "timestamp": datetime.now().isoformat(),
            "is_fallback": True,
            "error_reason": error_msg
        }

    async def generate_dual_stage_psychometric_analysis(
        self,
        user_input: str,
        session_history: List[Dict[str, Any]],
        client_profile: Dict[str, Any],
        additional_context: Optional[Dict[str, Any]] = None
    ) -> Optional[Dict[str, Any]]:
        """
        NOWA FUNKCJA: Dwuetapowa analiza psychometryczna z confidence scoring
        
        ETAP 1: WstÄ™pna analiza + samoocena pewnoÅ›ci AI
        ETAP 2A: JeÅ›li pewnoÅ›Ä‡ â‰¥75% â†’ PeÅ‚na analiza  
        ETAP 2B: JeÅ›li pewnoÅ›Ä‡ <75% â†’ Generowanie pytaÅ„ pomocniczych
        
        Args:
            additional_context: Kontekst z odpowiedzi na pytania pomocnicze
        """
        try:
            logger.info("ğŸ§  [DUAL STAGE] Rozpoczynam dwuetapowÄ… analizÄ™ psychometrycznÄ…...")
            
            # Zbuduj transkrypcjÄ™ z dodatkowym kontekstem
            conversation_transcript = self._build_enhanced_transcript(
                user_input, session_history, additional_context
            )
            
            # ETAP 1: WstÄ™pna analiza z confidence scoring
            user_prompt = f"""
TRANSKRYPCJA ROZMOWY + DODATKOWY KONTEKST:
{conversation_transcript}

Wykonaj DWUETAPOWÄ„ analizÄ™ zgodnie z instrukcjami w system prompt.
"""

            # WywoÅ‚aj AI z dwuetapowym promptem
            for attempt in range(self.max_retries):
                try:
                    logger.info(f"ğŸ”„ [DUAL STAGE] PrÃ³ba {attempt + 1}: WysyÅ‚anie do LLM...")
                    
                    llm_response = await asyncio.to_thread(
                        self._sync_ollama_call,
                        DUAL_STAGE_PSYCHOMETRIC_PROMPT,
                        user_prompt
                    )
                    
                    # Parsuj dwuetapowÄ… odpowiedÅº
                    parsed_response = self._parse_dual_stage_response(llm_response)
                    
                    if parsed_response:
                        confidence = parsed_response.get('confidence_score', 0)
                        needs_clarification = parsed_response.get('needs_clarification', True)
                        
                        logger.info(f"âœ… [DUAL STAGE] Analiza zakoÅ„czona: confidence={confidence}%, needs_clarification={needs_clarification}")
                        return parsed_response
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ [DUAL STAGE] PrÃ³ba {attempt + 1} nie powiodÅ‚a siÄ™: {e}")
                    if attempt < self.max_retries - 1:
                        await asyncio.sleep((attempt + 1) * 2)
            
            logger.error("âŒ [DUAL STAGE] Wszystkie prÃ³by analizy nie powiodÅ‚y siÄ™")
            return None
            
        except Exception as e:
            logger.error(f"âŒ [DUAL STAGE] BÅ‚Ä…d podczas dwuetapowej analizy: {e}")
            return None

    async def generate_psychometric_analysis(
        self,
        user_input: str,
        session_history: List[Dict[str, Any]],
        client_profile: Dict[str, Any],
        interactive_mode: bool = True
    ) -> Optional[Dict[str, Any]]:
        """
        Wygeneruj szczegÃ³Å‚owÄ… analizÄ™ psychometrycznÄ… klienta (ModuÅ‚ 2)
        
        To jest "wolna Å›cieÅ¼ka" analizy - wykonywana asynchronicznie, nie blokuje UI.
        Analizuje caÅ‚Ä… transkrypcjÄ™ rozmowy pod kÄ…tem cech Big Five, DISC i wartoÅ›ci Schwartza.
        
        Args:
            user_input: Aktualne wejÅ›cie od sprzedawcy
            session_history: Historia caÅ‚ej rozmowy w sesji
            client_profile: Profil klienta z dotychczasowymi informacjami
            interactive_mode: Czy AI moÅ¼e zadawaÄ‡ pytania gdy brak danych (default: True)
            
        Returns:
            SÅ‚ownik z peÅ‚nÄ… analizÄ… psychometrycznÄ…, probing questions, lub None w przypadku bÅ‚Ä™du
        """
        try:
            logger.info("ğŸ§  Rozpoczynam analizÄ™ psychometrycznÄ… klienta...")
            
            # Zbuduj peÅ‚nÄ… transkrypcjÄ™ rozmowy
            conversation_transcript = self._build_conversation_transcript(user_input, session_history)
            
            # SprawdÅº czy mamy wystarczajÄ…ce dane
            transcript_length = len(conversation_transcript)
            has_sufficient_data = transcript_length > 300 and len(session_history) >= 1  # Minimalne kryteria
            
            print(f"ğŸ§  Analiza psychometryczna: dÅ‚ugoÅ›Ä‡ transkrypcji = {transcript_length}, historia = {len(session_history)}")
            
            # Wybierz odpowiedni prompt
            if interactive_mode and not has_sufficient_data:
                system_prompt = DUAL_STAGE_PSYCHOMETRIC_PROMPT
                user_prompt = f"""
TRANSKRYPCJA ROZMOWY DO ANALIZY:
{conversation_transcript}

KONTEKST KLIENTA:
- Archetyp: {client_profile.get('archetype', 'Nieznany')}
- Notatki: {client_profile.get('notes', 'Brak')}

OceÅ„ czy masz wystarczajÄ…ce dane do peÅ‚nej analizy psychometrycznej, czy potrzebujesz wiÄ™cej informacji.
"""
            else:
                system_prompt = PSYCHOMETRIC_SYSTEM_PROMPT
                user_prompt = f"""
TRANSKRYPCJA ROZMOWY DO ANALIZY:
{conversation_transcript}

Przeanalizuj powyÅ¼szÄ… rozmowÄ™ sprzedaÅ¼owÄ… i stwÃ³rz kompletny profil psychometryczny klienta zgodnie z podanymi instrukcjami.
"""

            print(f"ğŸ§  UÅ¼ywam {'INTERACTIVE' if interactive_mode and not has_sufficient_data else 'STANDARD'} prompt")

            # WywoÅ‚aj LLM z odpowiednim promptem
            for attempt in range(self.max_retries):
                try:
                    logger.info(f"ğŸ”„ PrÃ³ba {attempt + 1}: WysyÅ‚anie zapytania o analizÄ™ psychometrycznÄ… do LLM...")
                    
                    llm_response = await asyncio.to_thread(
                        self._sync_ollama_call,
                        system_prompt,
                        user_prompt
                    )
                    
                    # Parsuj odpowiedÅº JSON
                    parsed_response = self._parse_psychometric_response(llm_response)
                    
                    if parsed_response:
                        logger.info("âœ… Analiza psychometryczna zakoÅ„czona pomyÅ›lnie!")
                        return parsed_response
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ PrÃ³ba {attempt + 1} nie powiodÅ‚a siÄ™: {e}")
                    if attempt < self.max_retries - 1:
                        await asyncio.sleep((attempt + 1) * 2)  # Exponential backoff
            
            logger.error("âŒ Wszystkie prÃ³by analizy psychometrycznej nie powiodÅ‚y siÄ™")
            return None
            
        except Exception as e:
            logger.error(f"âŒ BÅ‚Ä…d podczas analizy psychometrycznej: {e}")
            return None
    
    def _build_conversation_transcript(self, current_input: str, session_history: List[Dict[str, Any]]) -> str:
        """
        Zbuduj peÅ‚nÄ… transkrypcjÄ™ rozmowy dla analizy psychometrycznej
        """
        transcript = "=== TRANSKRYPCJA ROZMOWY SPRZEDAÅ»OWEJ ===\n\n"
        
        # Dodaj historiÄ™ interakcji
        for i, interaction in enumerate(session_history, 1):
            user_input = interaction.get('user_input', '')
            timestamp = interaction.get('timestamp', 'nieznany czas')
            
            transcript += f"[{i}] Sprzedawca ({timestamp}): \n{user_input}\n\n"
        
        # Dodaj aktualnÄ… interakcjÄ™
        transcript += f"[BIEÅ»Ä„CA] Sprzedawca: \n{current_input}\n\n"
        
        transcript += "=== KONIEC TRANSKRYPCJI ==="
        return transcript
    
    def _build_enhanced_transcript(
        self, 
        current_input: str, 
        session_history: List[Dict[str, Any]], 
        additional_context: Optional[Dict[str, Any]] = None
    ) -> str:
        """
        Buduje rozszerzonÄ… transkrypcjÄ™ z dodatkowym kontekstem z pytaÅ„ pomocniczych
        """
        transcript = self._build_conversation_transcript(current_input, session_history)
        
        # Dodaj kontekst z odpowiedzi na pytania pomocnicze
        if additional_context:
            transcript += "\n\n=== DODATKOWY KONTEKST Z OBSERWACJI SPRZEDAWCY ===\n"
            
            if 'clarifying_answers' in additional_context:
                transcript += "ODPOWIEDZI NA PYTANIA POMOCNICZE AI:\n"
                for answer in additional_context['clarifying_answers']:
                    question = answer.get('question', 'Nieznane pytanie')
                    selected_option = answer.get('selected_option', 'Brak odpowiedzi')
                    psychological_target = answer.get('psychological_target', '')
                    
                    transcript += f"- Pytanie: {question}\n"
                    transcript += f"  OdpowiedÅº: {selected_option}\n"
                    transcript += f"  Cel psychologiczny: {psychological_target}\n\n"
            
            transcript += "=== KONIEC DODATKOWEGO KONTEKSTU ==="
        
        return transcript
    
    def _parse_dual_stage_response(self, llm_response: str) -> Optional[Dict[str, Any]]:
        """
        Parsuj odpowiedÅº z dwuetapowej analizy psychometrycznej
        """
        try:
            cleaned_response = llm_response.strip()
            
            # ZnajdÅº JSON w odpowiedzi
            start_idx = cleaned_response.find('{')
            end_idx = cleaned_response.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                logger.warning("âš ï¸ [DUAL STAGE] Brak poprawnego JSON w odpowiedzi")
                return None
            
            json_str = cleaned_response[start_idx:end_idx]
            parsed_data = json.loads(json_str)
            
            # Walidacja podstawowych pÃ³l dwuetapowej analizy
            if 'confidence_score' not in parsed_data:
                logger.warning("âš ï¸ [DUAL STAGE] Brak confidence_score w odpowiedzi")
                return None
                
            confidence = parsed_data.get('confidence_score', 0)
            needs_clarification = confidence < 75  # Automatyczna logika decyzyjna
            
            # Aktualizuj flagÄ™ na podstawie confidence
            parsed_data['needs_clarification'] = needs_clarification
            
            if needs_clarification and 'clarifying_questions' not in parsed_data:
                logger.warning("âš ï¸ [DUAL STAGE] Niska pewnoÅ›Ä‡ ale brak pytaÅ„ pomocniczych")
                
            logger.info(f"âœ… [DUAL STAGE] Dwuetapowa odpowiedÅº sparsowana: confidence={confidence}%, needs_clarification={needs_clarification}")
            return parsed_data
            
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ [DUAL STAGE] BÅ‚Ä…d parsowania JSON: {e}")
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ [DUAL STAGE] Nieoczekiwany bÅ‚Ä…d podczas parsowania: {e}")
            return None
    
    def _parse_psychometric_response(self, llm_response: str) -> Optional[Dict[str, Any]]:
        """
        Parsuj odpowiedÅº LLM dla analizy psychometrycznej
        ObsÅ‚uguje zarÃ³wno peÅ‚nÄ… analizÄ™ jak i interactive mode
        """
        try:
            # WyczyÅ›Ä‡ odpowiedÅº z potencjalnych prefixÃ³w/sufiksÃ³w
            cleaned_response = llm_response.strip()
            
            # ZnajdÅº JSON w odpowiedzi
            start_idx = cleaned_response.find('{')
            end_idx = cleaned_response.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                logger.warning("âš ï¸ Brak poprawnego JSON w odpowiedzi psychometrycznej")
                return None
            
            json_str = cleaned_response[start_idx:end_idx]
            parsed_data = json.loads(json_str)
            
            # SprawdÅº czy to interactive mode response
            if parsed_data.get('insufficient_data') or parsed_data.get('mode') == 'interactive':
                logger.info("ğŸ“‹ Otrzymano interactive response z probing questions")
                return {
                    'mode': 'interactive',
                    'probing_questions': parsed_data.get('probing_questions', []),
                    'confidence_level': parsed_data.get('confidence_level', 'low'),
                    'next_steps': parsed_data.get('next_steps', ''),
                    'suggestions': parsed_data.get('suggestions', '')
                }
            
            # Walidacja struktury dla peÅ‚nej analizy
            required_keys = ['big_five', 'disc', 'schwartz_values']
            if not all(key in parsed_data for key in required_keys):
                logger.warning("âš ï¸ NiepeÅ‚na struktura w odpowiedzi psychometrycznej")
                return None
            
            logger.info("âœ… OdpowiedÅº psychometryczna zostaÅ‚a pomyÅ›lnie sparsowana")
            return parsed_data
            
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ BÅ‚Ä…d parsowania JSON w analizie psychometrycznej: {e}")
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ Nieoczekiwany bÅ‚Ä…d podczas parsowania analizy psychometrycznej: {e}")
            return None

    async def generate_psychologically_informed_response(
        self,
        user_input: str,
        client_profile: Dict[str, Any],
        psychometric_profile: Dict[str, Any],
        session_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        KROK 4: Generuje psychologicznie dostosowanÄ… sugerowanÄ… odpowiedÅº
        
        Wykorzystuje POTWIERDZONY profil psychometryczny do precyzyjnego 
        dostosowania tonu, stylu i treÅ›ci odpowiedzi.
        """
        try:
            logger.info("ğŸ­ [PSYCH RESPONSE] GenerujÄ™ psychologicznie dostosowanÄ… odpowiedÅº...")
            
            # Przygotuj kontekst psychometryczny dla AI
            psych_context = self._format_psychometric_context(psychometric_profile)
            
            user_prompt = f"""
SYTUACJA KLIENTA:
{user_input}

POTWIERDZONY PROFIL PSYCHOMETRYCZNY:
{psych_context}

KONTEKST KLIENTA:
- Archetyp: {client_profile.get('archetype', 'Nieznany')}
- Notatki: {client_profile.get('notes', 'Brak')}

Wygeneruj psychologicznie dostosowanÄ… sugerowanÄ… odpowiedÅº uwzglÄ™dniajÄ…c profil klienta.
"""

            for attempt in range(self.max_retries):
                try:
                    llm_response = await asyncio.to_thread(
                        self._sync_ollama_call,
                        PSYCHOLOGICALLY_INFORMED_RESPONSE_PROMPT,
                        user_prompt
                    )
                    
                    # Parsuj odpowiedÅº
                    parsed_response = self._parse_llm_response(llm_response)
                    
                    if parsed_response:
                        logger.info("âœ… [PSYCH RESPONSE] Psychologicznie dostosowana odpowiedÅº wygenerowana")
                        return parsed_response
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ [PSYCH RESPONSE] PrÃ³ba {attempt + 1} nie powiodÅ‚a siÄ™: {e}")
                    if attempt < self.max_retries - 1:
                        await asyncio.sleep((attempt + 1) * 2)
            
            # Fallback
            return self._create_fallback_response(user_input, "BÅ‚Ä…d generowania psychologicznie dostosowanej odpowiedzi")
            
        except Exception as e:
            logger.error(f"âŒ [PSYCH RESPONSE] BÅ‚Ä…d: {e}")
            return self._create_fallback_response(user_input, str(e))
    
    def _format_psychometric_context(self, psychometric_profile: Dict[str, Any]) -> str:
        """
        Formatuje profil psychometryczny dla AI prompt
        """
        context = ""
        
        if psychometric_profile.get('big_five'):
            context += "BIG FIVE PROFILE:\n"
            for trait, data in psychometric_profile['big_five'].items():
                score = data.get('score', 0)
                context += f"- {trait.title()}: {score}/10\n"
            context += "\n"
        
        if psychometric_profile.get('disc'):
            context += "DISC PROFILE:\n"
            for trait, data in psychometric_profile['disc'].items():
                score = data.get('score', 0)
                context += f"- {trait.title()}: {score}/10\n"
            context += "\n"
        
        if psychometric_profile.get('schwartz_values'):
            present_values = [v['value_name'] for v in psychometric_profile['schwartz_values'] if v.get('is_present')]
            if present_values:
                context += f"KLUCZOWE WARTOÅšCI: {', '.join(present_values)}\n"
        
        return context

    async def generate_psychology_enhanced_analysis(
        self,
        user_input: str,
        client_profile: Dict[str, Any],
        session_history: List[Dict[str, Any]],
        session_context: Optional[Dict[str, Any]] = None,
        session_psychology: Optional[Dict[str, Any]] = None,  # DEPRECATED v4.0
        holistic_profile: Optional[Dict[str, Any]] = None,    # NOWY v4.0: DNA Klienta
        customer_archetype: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ETAP 4 v3.0: Psychology-Enhanced Strategy Generation
        
        Generuje kompletnÄ… strategiÄ™ sprzedaÅ¼owÄ… uwzglÄ™dniajÄ…cÄ…:
        - Potwierdzony profil psychologiczny sesji
        - Customer archetype z kluczowymi poradami
        - Dostosowane suggested_actions i quick_response
        """
        try:
            logger.info("ğŸ­ [PSYCHOLOGY STRATEGY] GenerujÄ™ psychology-enhanced analysis...")
            
            # ULTRA MÃ“ZG v4.0: Priorytetyzujemy holistic_profile nad starymi danymi
            if holistic_profile:
                logger.info("ğŸ§ âš¡ [PSYCHOLOGY STRATEGY] UÅ¼ywam Ultra MÃ³zgu z DNA Klienta")
                return await self.generate_analysis(
                    user_input=user_input,
                    client_profile=client_profile, 
                    session_history=session_history,
                    session_context=session_context,
                    session_psychology=session_psychology,  # DEPRECATED ale zachowujemy dla kompatybilnoÅ›ci
                    holistic_profile=holistic_profile       # NOWY v4.0: DNA Klienta
                )
            elif session_psychology and customer_archetype:
                # DEPRECATED: Stara logika z archetyp-enhanced strategy
                logger.info("âš ï¸ [PSYCHOLOGY STRATEGY] Fallback: uÅ¼ywam archetype-enhanced strategy")
                return await self._generate_archetype_informed_strategy(
                    user_input, client_profile, session_psychology, customer_archetype
                )
            else:
                # Fallback do standardowej analizy
                logger.info("ğŸ­ [PSYCHOLOGY STRATEGY] Brak psychology data - fallback do standard analysis")
                return await self.generate_analysis(
                    user_input=user_input,
                    client_profile=client_profile, 
                    session_history=session_history,
                    session_context=session_context
                )
                
        except Exception as e:
            logger.error(f"âŒ [PSYCHOLOGY STRATEGY] Error: {e}")
            # Fallback do standardowej analizy
            return await self.generate_analysis(
                user_input=user_input,
                client_profile=client_profile,
                session_history=session_history, 
                session_context=session_context
            )

    async def _generate_archetype_informed_strategy(
        self,
        user_input: str,
        client_profile: Dict[str, Any],
        session_psychology: Dict[str, Any],
        customer_archetype: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ETAP 4: Generuje strategiÄ™ dostosowanÄ… do archetypu klienta
        """
        try:
            archetype_name = customer_archetype.get('archetype_name', 'Unknown')
            archetype_key = customer_archetype.get('archetype_key', 'unknown')
            sales_strategy = customer_archetype.get('sales_strategy', {})
            
            logger.info(f"ğŸ­ [ARCHETYPE STRATEGY] GenerujÄ™ strategiÄ™ dla archetypu: {archetype_name}")
            
            # Enhanced system prompt z archetype context
            psychology_informed_prompt = f"""
JesteÅ› ekspertem sprzedaÅ¼y Tesla generujÄ…cym PSYCHOLOGICZNIE DOSTOSOWANÄ„ strategiÄ™.

KLUCZOWE: Klient zostaÅ‚ zidentyfikowany jako ARCHETYP: {archetype_name}

ARCHETYPE PROFILE:
- Nazwa: {archetype_name}
- Kluczowe cechy: {customer_archetype.get('key_traits', [])}
- Strategia "RÃ“B TO": {sales_strategy.get('do', [])}
- Strategia "NIE RÃ“B TEGO": {sales_strategy.get('dont', [])}

PROFIL PSYCHOLOGICZNY SESJI:
{json.dumps(session_psychology, ensure_ascii=False, indent=2)}

TWOJE ZADANIE:
1. Wygeneruj main_analysis uwzglÄ™dniajÄ…cy archetyp klienta
2. StwÃ³rz quick_response dostosowanÄ… do archetypu (ton, styl, treÅ›Ä‡)
3. Zaproponuj suggested_actions zgodne ze strategiÄ… archetypu
4. OkreÅ›l next_best_action na podstawie psychologii klienta
5. Wygeneruj qualifying_questions ktÃ³re pogÅ‚Ä™biÄ… zrozumienie tego archetypu

KONTEKST KLIENTA:
- Alias: {client_profile.get('alias', 'Unknown')}
- Archetyp (stary): {client_profile.get('archetype', 'Unknown')}
- Notatki: {client_profile.get('notes', 'Brak')}

OBECNA SYTUACJA:
{user_input}

Wygeneruj odpowiedÅº w standardowym formacie JSON, ale DOSTOSOWANÄ„ do archetypu {archetype_name}.
"""

            # WywoÅ‚aj AI z psychology-informed promptem
            for attempt in range(self.max_retries):
                try:
                    llm_response = await asyncio.to_thread(
                        self._sync_ollama_call,
                        psychology_informed_prompt,
                        "Wygeneruj psychology-enhanced strategy zgodnie z instrukcjami."
                    )
                    
                    # Parsuj i enhanced response
                    parsed_response = self._parse_llm_response(llm_response)
                    
                    if parsed_response:
                        # Dodaj archetype info do response
                        parsed_response['customer_archetype'] = customer_archetype
                        parsed_response['psychology_enhanced'] = True
                        parsed_response['confidence_level'] = session_psychology.get('confidence', 0)
                        
                        logger.info(f"âœ… [ARCHETYPE STRATEGY] Strategy wygenerowana dla {archetype_name}")
                        return parsed_response
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ [ARCHETYPE STRATEGY] PrÃ³ba {attempt + 1} failed: {e}")
                    if attempt < self.max_retries - 1:
                        await asyncio.sleep((attempt + 1) * 2)
            
            # Fallback jeÅ›li AI failures
            return self._create_archetype_fallback_response(user_input, customer_archetype)
            
        except Exception as e:
            logger.error(f"âŒ [ARCHETYPE STRATEGY] Error: {e}")
            return self._create_archetype_fallback_response(user_input, customer_archetype)

    def _create_archetype_fallback_response(self, user_input: str, archetype: Dict) -> Dict[str, Any]:
        """Fallback response z archetype info"""
        archetype_name = archetype.get('archetype_name', 'Klient')
        sales_strategy = archetype.get('sales_strategy', {})
        
        return {
            "main_analysis": f"Rozmawiasz z klientem typu {archetype_name}. Dostosuj podejÅ›cie do jego profilu psychologicznego.",
            "quick_response": {
                "id": "archetype_fallback",
                "text": f"Na podstawie Twojego profilu jako {archetype_name}, sugerujÄ™..."
            },
            "suggested_actions": [
                {"action": action, "reasoning": f"Strategia dla {archetype_name}"}
                for action in sales_strategy.get('do', ['Dostosuj podejÅ›cie do klienta'])[:3]
            ],
            "next_best_action": f"Zastosuj strategiÄ™ dla {archetype_name}",
            "customer_archetype": archetype,
            "psychology_enhanced": True,
            "is_fallback": True
        }

    async def _run_holistic_synthesis(self, raw_psychology_profile: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ§  ULTRA MÃ“ZG FAZA 2 - SYNTEZATOR PROFILU HOLISTYCZNEGO
        
        PrzeksztaÅ‚ca surowe dane psychometryczne w jeden, spÃ³jny profil strategiczny.
        To jest pierwszy "neuron" Ultra MÃ³zgu - gÅ‚Ä™boka analiza i synteza.
        
        Args:
            raw_psychology_profile: Surowe dane z SessionPsychologyEngine
            
        Returns:
            dict: Holistyczny profil strategyczny - "DNA Klienta"
        """
        try:
            logger.info("ğŸ”¬ [SYNTEZATOR] Rozpoczynam syntezÄ™ holistycznego profilu klienta...")
            
            # Przygotuj dane wejÅ›ciowe do syntezy
            profile_json = json.dumps(raw_psychology_profile, ensure_ascii=False, indent=2)
            
            # PROMPT SYNTEZY - Åšwiatowej klasy psycholog biznesu
            synthesis_prompt = f"""
JesteÅ› Å›wiatowej klasy psychologiem biznesu i analitykiem behawioralnym. Twoim zadaniem jest synteza surowych danych z wielu testÃ³w psychometrycznych w jeden, spÃ³jny i uÅ¼yteczny profil strategiczny.

Przeanalizuj poniÅ¼sze dane. Zidentyfikuj kluczowe wzorce, synergie i potencjalne sprzecznoÅ›ci. Twoim celem jest stworzenie skondensowanego "DNA Klienta", ktÃ³re posÅ‚uÅ¼y strategowi sprzedaÅ¼y do podjÄ™cia dalszych dziaÅ‚aÅ„.

SUROWE DANE PSYCHOMETRYCZNE:
{profile_json}

ZwrÃ³Ä‡ odpowiedÅº wyÅ‚Ä…cznie w formacie JSON o nastÄ™pujÄ…cej strukturze:

{{
  "holistic_summary": "Jednozdaniowe, esencjonalne podsumowanie klienta, np. 'Analityczny decydent motywowany statusem i bezpieczeÅ„stwem, nieufny wobec emocjonalnych argumentÃ³w.'",
  "main_drive": "GÅ‚Ã³wny, podÅ›wiadomy motor napÄ™dowy klienta, np. 'Unikanie ryzyka', 'DÄ…Å¼enie do dominacji', 'Potrzeba akceptacji'",
  "communication_style": {{
    "recommended_tone": "np. 'Formalny, oparty na danych, zwiÄ™zÅ‚y'",
    "keywords_to_use": ["np. 'dowÃ³d', 'gwarancja', 'efektywnoÅ›Ä‡', 'plan'"],
    "keywords_to_avoid": ["np. 'uczucie', 'wyobraÅº sobie', 'zaufaj mi'"]
  }},
  "key_levers": ["Dwie lub trzy najwaÅ¼niejsze 'dÅºwignie' psychologiczne, na ktÃ³re naleÅ¼y nacisnÄ…Ä‡, np. 'OdwoÅ‚anie do statusu eksperta', 'PodkreÅ›lenie bezpieczeÅ„stwa inwestycji'"],
  "red_flags": ["Czego absolutnie unikaÄ‡ w kontakcie, np. 'Pospieszania decyzji', 'Stosowania nieformalnego jÄ™zyka', 'PodwaÅ¼ania jego wiedzy'"],
  "missing_data_gaps": "Jakich kluczowych informacji brakuje, aby ten profil byÅ‚ peÅ‚niejszy? SformuÅ‚uj to jako cel dla sprzedawcy, np. 'NaleÅ¼y zidentyfikowaÄ‡ jego osobisty stosunek do ryzyka finansowego.'"
}}

KRYTYCZNE: ZwrÃ³Ä‡ WYÅÄ„CZNIE poprawny JSON bez dodatkowego tekstu.
"""
            
            # WywoÅ‚aj AI z promptem syntezy
            logger.info("ğŸ¤– [SYNTEZATOR] WysyÅ‚am dane do AI w celu syntezy...")
            ai_response = await self._call_llm_with_retry(
                system_prompt="JesteÅ› ekspertem syntezy profili psychologicznych.",
                user_prompt=synthesis_prompt
            )
            
            # Parsuj odpowiedÅº AI
            logger.info("ğŸ“Š [SYNTEZATOR] Parsowanie odpowiedzi AI...")
            holistic_profile = self._parse_holistic_synthesis_response(ai_response)
            
            if not holistic_profile:
                logger.warning("âš ï¸ [SYNTEZATOR] Parsowanie nie powiodÅ‚o siÄ™, uÅ¼ywam fallback")
                holistic_profile = self._create_fallback_holistic_profile(raw_psychology_profile)
            
            logger.info(f"âœ… [SYNTEZATOR] Synteza holistyczna ukoÅ„czona! GÅ‚Ã³wny drive: {holistic_profile.get('main_drive', 'Unknown')}")
            
            return holistic_profile
            
        except Exception as e:
            logger.error(f"âŒ [SYNTEZATOR] BÅ‚Ä…d podczas syntezy holistycznej: {e}")
            return self._create_fallback_holistic_profile(raw_psychology_profile)

    def _parse_holistic_synthesis_response(self, ai_response: str) -> Optional[Dict[str, Any]]:
        """Parsuje odpowiedÅº AI z syntezy holistycznej"""
        try:
            # ZnajdÅº JSON w odpowiedzi
            start_idx = ai_response.find('{')
            end_idx = ai_response.rfind('}') + 1
            
            if start_idx == -1 or end_idx <= start_idx:
                logger.warning("âš ï¸ [SYNTEZATOR PARSER] Brak JSON w odpowiedzi AI")
                return None
                
            json_str = ai_response[start_idx:end_idx]
            parsed_data = json.loads(json_str)
            
            # Waliduj wymagane pola
            required_fields = ['holistic_summary', 'main_drive', 'communication_style', 'key_levers', 'red_flags']
            for field in required_fields:
                if field not in parsed_data:
                    logger.warning(f"âš ï¸ [SYNTEZATOR PARSER] Brakuje pola: {field}")
                    return None
            
            logger.info("âœ… [SYNTEZATOR PARSER] JSON parsed successfully")
            return parsed_data
            
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ [SYNTEZATOR PARSER] JSON decode error: {e}")
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ [SYNTEZATOR PARSER] Unexpected error: {e}")
            return None

    def _create_fallback_holistic_profile(self, raw_psychology_profile: Dict[str, Any]) -> Dict[str, Any]:
        """Tworzy podstawowy profil holistyczny gdy AI nie jest dostÄ™pny"""
        
        # SprÃ³buj wyciÄ…gnÄ…Ä‡ podstawowe informacje z raw profile
        archetype = raw_psychology_profile.get('customer_archetype', {})
        archetype_name = archetype.get('archetype_name', 'â“ Profil w Trakcie Analizy')
        confidence = raw_psychology_profile.get('psychology_confidence', 0)
        
        return {
            "holistic_summary": f"Klient typu {archetype_name} z {confidence}% poziomem pewnoÅ›ci profilu. Analiza wymaga wiÄ™cej danych.",
            "main_drive": "Potrzeba zrozumienia i kontroli sytuacji zakupowej",
            "communication_style": {
                "recommended_tone": "Profesjonalny, oparty na faktach, cierpliwy",
                "keywords_to_use": ["informacje", "opcje", "korzyÅ›ci", "rozwiÄ…zanie"],
                "keywords_to_avoid": ["poÅ›piech", "presja", "ograniczona oferta"]
            },
            "key_levers": [
                "Dostarczenie szczegÃ³Å‚owych informacji",
                "Budowanie zaufania przez transparentnoÅ›Ä‡",
                "Pokazanie konkretnych korzyÅ›ci"
            ],
            "red_flags": [
                "Wywieranie presji czasowej",
                "Pomijanie pytaÅ„ klienta",
                "Zbyt agresywne podejÅ›cie sprzedaÅ¼owe"
            ],
            "missing_data_gaps": "Potrzeba wiÄ™cej informacji o motywacjach, preferencjach komunikacyjnych i procesie podejmowania decyzji klienta.",
            "is_fallback": True,
            "fallback_reason": "AI synthesis unavailable"
        }

    async def _run_strategic_generator(
        self,
        user_input: str,
        client_profile: Dict[str, Any],
        holistic_profile: Dict[str, Any],
        session_history: List[Dict[str, Any]],
        session_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        âš¡ ULTRA MÃ“ZG FAZA 2 - GENERATOR STRATEGII
        
        Drugi "neuron" Ultra MÃ³zgu - generuje precyzyjne wsparcie taktyczne
        na podstawie DNA Klienta (holistic_profile).
        
        Args:
            user_input: Ostatnia wypowiedÅº/obserwacja sprzedawcy
            client_profile: Podstawowe dane klienta
            holistic_profile: DNA Klienta z Syntezatora
            session_history: Historia rozmowy
            session_context: Kontekst sesji
            
        Returns:
            dict: Strategiczna odpowiedÅº w formacie InteractionResponse
        """
        try:
            logger.info("âš¡ [GENERATOR STRATEGII] Rozpoczynam generacjÄ™ strategii na podstawie DNA...")
            
            # Przygotuj DNA Klienta do analizy
            dna_json = json.dumps(holistic_profile, ensure_ascii=False, indent=2)
            
            # Historia rozmowy w czytelnym formacie
            history_str = self._format_session_history_for_strategy(session_history)
            
            # PROMPT STRATEGICZNY - Elitarny co-pilot sprzedaÅ¼y
            strategic_prompt = f"""
JesteÅ› elitarnym co-pilotem sprzedaÅ¼y Tesla. OtrzymaÅ‚eÅ› kompletne DNA klienta. Twoim zadaniem jest, na podstawie tego profilu oraz ostatniej wypowiedzi klienta, wygenerowaÄ‡ natychmiastowe, taktyczne wsparcie dla sprzedawcy.

DNA KLIENTA (PROFIL HOLISTYCZNY):
{dna_json}

HISTORIA ROZMOWY:
{history_str}

OSTATNIA WYPOWIEDÅ¹ SPRZEDAWCY:
"{user_input}"

Na podstawie powyÅ¼szego DNA Klienta oraz ostatniej wypowiedzi, wygeneruj odpowiedÅº wyÅ‚Ä…cznie w formacie JSON o nastÄ™pujÄ…cej strukturze:

{{
  "main_analysis": "KrÃ³tka analiza sytuacji na podstawie DNA klienta i wypowiedzi",
  "client_archetype": "Nazwa archetypu z DNA",
  "confidence_level": 85,
  "sentiment_score": 7,
  "potential_score": 8,
  "urgency_level": "medium",
  "next_best_action": "Konkretna akcja dla sprzedawcy oparta na DNA",
  "quick_response": {{
    "id": "qr_xyz123",
    "text": "Sugerowana odpowiedÅº dopasowana do communication_style z DNA"
  }},
  "suggested_questions": [
    {{
      "id": "sq_abc123", 
      "text": "Pytanie ktÃ³re wykorzystuje key_levers z DNA i unika red_flags"
    }}
  ],
  "strategic_recommendation": "Rekomendacja strategiczna na ten moment rozmowy, oparta na main_drive z DNA",
  "proactive_guidance": {{
    "for_client": "Pytanie do klienta ktÃ³re pomoÅ¼e wypeÅ‚niÄ‡ missing_data_gaps z DNA",
    "for_user": "Pytanie do sprzedawcy o obserwacje klienta"
  }},
  "strategic_notes": [
    "Kluczowy insight oparty na DNA klienta",
    "Druga strategiczna obserwacja z holistic_summary"
  ]
}}

KLUCZOWE ZASADY:
1. UÅ»YJ main_drive z DNA jako gÅ‚Ã³wnej motywacji w odpowiedzi
2. ZASTOSUJ communication_style (tone, keywords_to_use, unikaj keywords_to_avoid)  
3. WYKORZYSTAJ key_levers jako gÅ‚Ã³wne argumenty
4. UNIKAJ red_flags za wszelkÄ… cenÄ™
5. WYPEÅNIJ missing_data_gaps przez proactive_guidance

KRYTYCZNE: ZwrÃ³Ä‡ WYÅÄ„CZNIE poprawny JSON bez dodatkowego tekstu.
"""
            
            # WywoÅ‚aj AI z promptem strategicznym
            logger.info("ğŸ¤– [GENERATOR STRATEGII] WysyÅ‚am DNA + kontekst do AI...")
            ai_response = await self._call_llm_with_retry(
                system_prompt="JesteÅ› elitarnym co-pilotem sprzedaÅ¼y Tesla uÅ¼ywajÄ…cym DNA klienta.",
                user_prompt=strategic_prompt
            )
            
            # Parsuj odpowiedÅº AI  
            logger.info("ğŸ“Š [GENERATOR STRATEGII] Parsowanie strategicznej odpowiedzi...")
            strategic_analysis = self._parse_strategic_response(ai_response)
            
            if not strategic_analysis:
                logger.warning("âš ï¸ [GENERATOR STRATEGII] Parsowanie nie powiodÅ‚o siÄ™, uÅ¼ywam fallback")
                strategic_analysis = self._create_strategic_fallback(user_input, holistic_profile)
            
            logger.info(f"âœ… [GENERATOR STRATEGII] Strategia gotowa! Action: {strategic_analysis.get('next_best_action', 'Unknown')}")
            
            return strategic_analysis
            
        except Exception as e:
            logger.error(f"âŒ [GENERATOR STRATEGII] BÅ‚Ä…d podczas generacji strategii: {e}")
            return self._create_strategic_fallback(user_input, holistic_profile)

    def _format_session_history_for_strategy(self, session_history: List[Dict[str, Any]]) -> str:
        """Formatuje historiÄ™ sesji dla promptu strategicznego"""
        if not session_history:
            return "Brak poprzedniej historii rozmowy."
        
        history_parts = []
        for i, interaction in enumerate(session_history[-3:], 1):  # Ostatnie 3 interakcje
            user_input = interaction.get('user_input', '')[:200]  # SkrÃ³Ä‡ dÅ‚ugie teksty
            timestamp = interaction.get('timestamp', 'nieznany czas')
            history_parts.append(f"{i}. [{timestamp}] Sprzedawca: \"{user_input}\"")
        
        return "\n".join(history_parts)

    def _parse_strategic_response(self, ai_response: str) -> Optional[Dict[str, Any]]:
        """Parsuje odpowiedÅº AI z generatora strategii"""
        try:
            # ZnajdÅº JSON w odpowiedzi
            start_idx = ai_response.find('{')
            end_idx = ai_response.rfind('}') + 1
            
            if start_idx == -1 or end_idx <= start_idx:
                logger.warning("âš ï¸ [STRATEGY PARSER] Brak JSON w odpowiedzi AI")
                return None
                
            json_str = ai_response[start_idx:end_idx]
            parsed_data = json.loads(json_str)
            
            # Waliduj wymagane pola dla InteractionResponse
            required_fields = [
                'main_analysis', 'client_archetype', 'confidence_level',
                'sentiment_score', 'potential_score', 'urgency_level', 'next_best_action'
            ]
            for field in required_fields:
                if field not in parsed_data:
                    logger.warning(f"âš ï¸ [STRATEGY PARSER] Brakuje pola: {field}")
                    return None
            
            logger.info("âœ… [STRATEGY PARSER] Strategiczna odpowiedÅº sparsowana")
            return parsed_data
            
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ [STRATEGY PARSER] JSON decode error: {e}")
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ [STRATEGY PARSER] Unexpected error: {e}")
            return None

    def _create_strategic_fallback(self, user_input: str, holistic_profile: Dict[str, Any]) -> Dict[str, Any]:
        """Tworzy fallback odpowiedÅº strategicznÄ… gdy AI nie jest dostÄ™pny"""
        
        # WyciÄ…gnij podstawowe info z DNA
        main_drive = holistic_profile.get('main_drive', 'Potrzeba zrozumienia sytuacji')
        archetype_name = holistic_profile.get('holistic_summary', 'Klient w trakcie analizy')
        
        return {
            "main_analysis": f"AnalizujÄ™ sytuacjÄ™ na podstawie DNA klienta. GÅ‚Ã³wny drive: {main_drive}",
            "client_archetype": archetype_name,
            "confidence_level": 50,
            "sentiment_score": 6,
            "potential_score": 7,
            "urgency_level": "medium",
            "next_best_action": "Kontynuuj budowanie zaufania i zbieranie informacji o potrzebach klienta",
            "quick_response": {
                "id": f"qr_{uuid.uuid4().hex[:6]}",
                "text": "Rozumiem. Czy mÃ³gÅ‚by Pan powiedzieÄ‡ wiÄ™cej o swoich oczekiwaniach?"
            },
            "suggested_questions": [
                {
                    "id": f"sq_{uuid.uuid4().hex[:6]}",
                    "text": "Jakie sÄ… najwaÅ¼niejsze kryteria w Pana decyzji?"
                }
            ],
            "strategic_recommendation": "Skup siÄ™ na budowaniu zaufania i zrozumieniu potrzeb klienta.",
            "strategic_notes": [
                "Klient wymaga cierpliwego podejÅ›cia",
                "Potrzebne wiÄ™cej informacji o motywacjach"
            ],
            "is_fallback": True,
            "fallback_reason": "Strategic AI generator unavailable"
        }


# Import Qdrant service for singleton creation
from .qdrant_service import qdrant_service

# Singleton instance z integracjÄ… RAG
ai_service = AIService(qdrant_service=qdrant_service)


# Helper funkcje dla Å‚atwego importu
async def generate_sales_analysis(
    user_input: str,
    client_profile: Dict[str, Any],
    session_history: List[Dict[str, Any]],
    session_context: Optional[Dict[str, Any]] = None,
    session_psychology: Optional[Dict[str, Any]] = None,  # DEPRECATED v4.0: UÅ¼ywaj holistic_profile
    holistic_profile: Optional[Dict[str, Any]] = None,    # NOWY v4.0: DNA Klienta z Syntezatora
    customer_archetype: Optional[Dict[str, Any]] = None   # NOWY v3.0: Archetyp klienta
) -> Dict[str, Any]:
    """
    ENHANCED v3.0: Wygeneruj analizÄ™ sprzedaÅ¼owÄ… z psychology-informed strategy
    
    Funkcja zostaÅ‚a rozszerzona o session-level psychology i customer archetype
    ktÃ³re wpÅ‚ywajÄ… na generowanÄ… strategiÄ™ i sugerowane odpowiedzi.
    """
    return await ai_service.generate_psychology_enhanced_analysis(
        user_input=user_input,
        client_profile=client_profile,
        session_history=session_history,
        session_context=session_context,
        session_psychology=session_psychology,  # DEPRECATED v4.0
        holistic_profile=holistic_profile,      # NOWY v4.0: DNA Klienta
        customer_archetype=customer_archetype
    )


async def generate_psychometric_analysis(
    user_input: str,
    session_history: List[Dict[str, Any]],
    client_profile: Dict[str, Any],
    interactive_mode: bool = True
) -> Optional[Dict[str, Any]]:
    """
    Wygeneruj analizÄ™ psychometrycznÄ… klienta - funkcja eksportowa dla ModuÅ‚u 2
    
    Args:
        interactive_mode: JeÅ›li True, AI moÅ¼e zadawaÄ‡ pytania gdy brak danych
    """
    return await ai_service.generate_psychometric_analysis(
        user_input=user_input,
        session_history=session_history,
        client_profile=client_profile,
        interactive_mode=interactive_mode
    )


async def generate_enhanced_psychometric_questions(
    user_input: str,
    session_context: Dict[str, Any],
    client_profile: Dict[str, Any]
) -> List[str]:
    """
    Wygeneruj strategiczne pytania dla lepszego zrozumienia psychologii klienta
    Gdy standardowa analiza nie daje wystarczajÄ…cych danych
    """
    try:
        prompt = f"""
JesteÅ› ekspertem psychologii sprzedaÅ¼y. Na podstawie bieÅ¼Ä…cej sytuacji wygeneruj 3-5 strategicznych pytaÅ„ 
ktÃ³re pomogÄ… sprzedawcy Tesla lepiej zrozumieÄ‡ psychologiÄ™ klienta.

SYTUACJA:
- Input sprzedawcy: "{user_input}"
- Archetyp klienta: {client_profile.get('archetype', 'Nieznany')}
- Kontekst: {session_context}

CELE PYTAÅƒ:
1. Zrozumienie stylu podejmowania decyzji (analityczny vs impulsywny)
2. Identyfikacja gÅ‚Ã³wnych motywatorÃ³w (status, bezpieczeÅ„stwo, technologia, ekologia)
3. Wykrycie obaw i potencjalnych zastrzeÅ¼eÅ„
4. OkreÅ›lenie stylu komunikacji

ZwrÃ³Ä‡ JSON:
{
  "probing_questions": [
    "Pytanie o styl decyzyjny...",
    "Pytanie o motywacje...",
    "Pytanie o obawy...",
    "Pytanie o komunikacjÄ™..."
  ]
}
"""

        response = await ai_service._call_llm_with_retry(prompt, "")
        
        try:
            start_idx = response.find('{')
            end_idx = response.rfind('}') + 1
            json_str = response[start_idx:end_idx]
            parsed = json.loads(json_str)
            return parsed.get('probing_questions', [])
        except:
            return [
                "Jakie czynniki sÄ… dla Pana najwaÅ¼niejsze przy wyborze samochodu?",
                "Czy preferuje Pan podejmowaÄ‡ decyzje szybko czy wolÄ… dokÅ‚adnie przeanalizowaÄ‡ opcje?",
                "Jakie sÄ… Pana gÅ‚Ã³wne obawy zwiÄ…zane z samochodami elektrycznymi?",
                "Czy waÅ¼ne jest dla Pana co pomyÅ›lÄ… inni o PaÅ„skim samochodzie?"
            ]
    except Exception as e:
        logger.warning(f"BÅ‚Ä…d podczas generowania enhanced questions: {e}")
        return []

    async def _run_sales_indicators_generation(self, holistic_profile: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ§ âš¡ ULTRA MÃ“ZG v4.1 - PRIORYTET 3: Generator WskaÅºnikÃ³w SprzedaÅ¼owych (z CACHING)
        
        Zgodnie z blueprintem - generuje wskaÅºniki sprzedaÅ¼owe na podstawie 
        holistycznego profilu psychologicznego (DNA Klienta) z inteligentnym cachingiem.
        
        Args:
            holistic_profile: Kompletne DNA Klienta z _run_holistic_synthesis
            
        Returns:
            dict: Obiekkt JSON zgodny ze schematem SalesIndicators
        """
        try:
            logger.info("ğŸ“Š [SALES INDICATORS] Rozpoczynam generacjÄ™ wskaÅºnikÃ³w na podstawie DNA...")
            
            # ğŸš€ PRIORYTET 4: SprawdÅº cache przed analizÄ… AI
            cache_key = self._generate_cache_key(holistic_profile, "indicators")
            cached_result = self._get_from_cache(self._sales_indicators_cache, cache_key)
            
            if cached_result:
                logger.info(f"âš¡ [SALES INDICATORS] Cache hit! Zwracam cached wskaÅºniki (oszczÄ™dnoÅ›Ä‡ ~5-8s)")
                cached_result['cache_hit'] = True
                cached_result['analysis_timestamp'] = datetime.now().isoformat()
                return cached_result
            
            logger.info(f"ğŸ” [SALES INDICATORS] Cache miss, przeprowadzam AI generation...")
            
            # WydobÄ…dÅº kluczowe informacje z DNA Klienta
            holistic_summary = holistic_profile.get('holistic_summary', 'Brak podsumowania')
            main_drive = holistic_profile.get('main_drive', 'NieokreÅ›lone')
            communication_style = holistic_profile.get('communication_style', {})
            key_levers = holistic_profile.get('key_levers', [])
            red_flags = holistic_profile.get('red_flags', [])
            confidence = holistic_profile.get('confidence', 0)
            
            # StwÃ³rz dedykowany prompt dla generacji wskaÅºnikÃ³w sprzedaÅ¼owych
            sales_indicators_prompt = f"""
JesteÅ› ekspertem analizy sprzedaÅ¼y z 15-letnim doÅ›wiadczeniem w ocenie potencjaÅ‚u klientÃ³w. 
Twoim zadaniem jest przeÅ‚oÅ¼enie DOKÅADNIE TEGO SAMEGO profilu psychologicznego na 4 precyzyjne wskaÅºniki sprzedaÅ¼owe.

ğŸ§  DNA KLIENTA DO ANALIZY:
PODSUMOWANIE: {holistic_summary}
GÅÃ“WNA MOTYWACJA: {main_drive}  
STYL KOMUNIKACJI: {json.dumps(communication_style, ensure_ascii=False)}
KLUCZOWE DÅ¹WIGNIE: {key_levers}
CZERWONE FLAGI: {red_flags}
PEWNOÅšÄ† PROFILU: {confidence}%

ZADANIE: PrzeksztaÅ‚Ä‡ ten profil psychologiczny na 4 wskaÅºniki sprzedaÅ¼owe:

1. ğŸŒ¡ï¸ TEMPERATURA ZAKUPOWA (0-100%):
   - Analityk: SzczegÃ³Å‚owe pytania = wysoka temperatura (pozytywne)
   - Szybki Decydent: SzczegÃ³Å‚owe pytania = wahanie (negatywne) 
   - Bazuj na main_drive i communication_style

2. ğŸ—ºï¸ ETAP PODRÃ“Å»Y:
   - awareness/consideration/evaluation/decision/purchase
   - Mapuj na typowy proces decyzyjny dla tego archetypu

3. âš–ï¸ RYZYKO UTRATY (0-100%):
   - OceÅ„ przez pryzmat red_flags i communication_style
   - NiÅ¼sze ryzyko = wiÄ™cej key_levers, mniej red_flags

4. ğŸ’° POTENCJAÅ SPRZEDAÅ»OWY:
   - Szacuj wartoÅ›Ä‡ na podstawie archetypu i poziomu zaangaÅ¼owania
   - B2B Tesla fleet: 100k-10M PLN, B2C: 50k-500k PLN

ğŸ¯ KRYTYCZNE: Wszystkie 4 wskaÅºniki muszÄ… byÄ‡ wzajemnie spÃ³jne i logiczne!

ZwrÃ³Ä‡ WYÅÄ„CZNIE JSON:
{{
    "purchase_temperature": {{
        "value": 75,
        "temperature_level": "hot",
        "rationale": "SzczegÃ³Å‚owa analiza profilu wskazuje na...",
        "strategy": "Rekomendowana akcja sprzedaÅ¼owa...",
        "confidence": 85
    }},
    "customer_journey_stage": {{
        "value": "evaluation",
        "progress_percentage": 70,
        "next_stage": "decision", 
        "rationale": "Na podstawie archetypu klient jest na etapie...",
        "strategy": "Zalecane dziaÅ‚anie: ...",
        "confidence": 90
    }},
    "churn_risk": {{
        "value": 25,
        "risk_level": "low",
        "risk_factors": ["Czynnik 1", "Czynnik 2"],
        "rationale": "Ryzyko ocenione na podstawie...",
        "strategy": "Strategia retencji: ...",
        "confidence": 80
    }},
    "sales_potential": {{
        "value": 450000.0,
        "probability": 75,
        "estimated_timeframe": "3-4 tygodnie", 
        "rationale": "WartoÅ›Ä‡ oszacowana na podstawie...",
        "strategy": "Plan dziaÅ‚ania: ...",
        "confidence": 85
    }}
}}
"""
            
            # WywoÅ‚aj AI z dedykowanym promptem
            logger.info("ğŸ¤– [SALES INDICATORS] WysyÅ‚am prompt do AI...")
            ai_response = await self._call_llm_with_retry(
                system_prompt="JesteÅ› ekspertem analizy sprzedaÅ¼y generujÄ…cym precyzyjne wskaÅºniki.",
                user_prompt=sales_indicators_prompt
            )
            
            # Parsuj odpowiedÅº AI
            parsed_indicators = self._parse_sales_indicators_response(ai_response)
            if not parsed_indicators:
                logger.warning("âš ï¸ [SALES INDICATORS] AI parsing failed, uÅ¼ywam fallback")
                return self._create_fallback_sales_indicators()
            
            # Dodaj metadata
            parsed_indicators['analysis_timestamp'] = datetime.now().isoformat()
            parsed_indicators['source_confidence'] = confidence
            parsed_indicators['cache_hit'] = False
            
            # ğŸš€ PRIORYTET 4: Zapisz do cache
            self._save_to_cache(self._sales_indicators_cache, cache_key, parsed_indicators)
            
            logger.info(f"âœ… [SALES INDICATORS] WskaÅºniki wygenerowane i cached pomyÅ›lnie")
            
            return parsed_indicators
            
        except Exception as e:
            logger.error(f"âŒ [SALES INDICATORS] BÅ‚Ä…d podczas generacji: {e}")
            return self._create_fallback_sales_indicators()

    def _parse_sales_indicators_response(self, ai_response: str) -> Optional[Dict[str, Any]]:
        """Parsuje odpowiedÅº AI z analizÄ… wskaÅºnikÃ³w sprzedaÅ¼owych"""
        try:
            # ZnajdÅº JSON w odpowiedzi
            start_idx = ai_response.find('{')
            end_idx = ai_response.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                logger.warning("âš ï¸ [SALES INDICATORS PARSE] Brak JSON w odpowiedzi AI")
                return None
                
            json_str = ai_response[start_idx:end_idx]
            parsed_data = json.loads(json_str)
            
            # Walidacja podstawowych pÃ³l
            required_fields = ['purchase_temperature', 'customer_journey_stage', 'churn_risk', 'sales_potential']
            for field in required_fields:
                if field not in parsed_data:
                    logger.warning(f"âš ï¸ [SALES INDICATORS PARSE] Brakuje pola: {field}")
                    return None
                    
            logger.info("âœ… [SALES INDICATORS PARSE] PomyÅ›lnie sparsowano wskaÅºniki")
            return parsed_data
            
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ [SALES INDICATORS PARSE] JSON decode error: {e}")
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ [SALES INDICATORS PARSE] Unexpected error: {e}")
            return None

    async def _run_holistic_synthesis(self, raw_psychology_profile: Dict[str, Any]) -> Dict[str, Any]:
        """
        ğŸ§ âš¡ ULTRA MÃ“ZG v4.1 - FAZA 2: Syntezator Profilu Holistycznego (z CACHING)
        
        Zgodnie z blueprintem - przeksztaÅ‚ca surowe dane psychometryczne w spÃ³jny
        "DNA Klienta" (holistic profile) z inteligentnym cachingiem.
        
        Args:
            raw_psychology_profile: Surowe dane psychology z session_psychology_service
            
        Returns:
            dict: Holistyczny profil klienta (DNA Klienta)
        """
        try:
            logger.info("ğŸ”¬ [HOLISTIC SYNTHESIS] Rozpoczynam syntezÄ™ DNA Klienta...")
            
            # ğŸš€ PRIORYTET 4: SprawdÅº cache przed analizÄ… AI
            cache_key = self._generate_cache_key(raw_psychology_profile, "synthesis")
            cached_result = self._get_from_cache(self._holistic_synthesis_cache, cache_key)
            
            if cached_result:
                logger.info(f"âš¡ [HOLISTIC SYNTHESIS] Cache hit! Zwracam cached DNA Klienta (oszczÄ™dnoÅ›Ä‡ ~10-15s)")
                cached_result['cache_hit'] = True
                cached_result['analysis_timestamp'] = datetime.now().isoformat()
                return cached_result
            
            logger.info(f"ğŸ” [HOLISTIC SYNTHESIS] Cache miss, przeprowadzam AI synthesis...")
            
            # WydobÄ…dÅº dane z raw psychology
            cumulative_psychology = raw_psychology_profile.get('cumulative_psychology', {})
            customer_archetype = raw_psychology_profile.get('customer_archetype', {})
            confidence = raw_psychology_profile.get('psychology_confidence', 0)
            
            # StwÃ³rz dedykowany prompt dla syntezy holistycznej
            synthesis_prompt = f"""
JesteÅ› Å›wiatowej klasy psychologiem biznesu i analitykiem behawioralnym. 
Twoim zadaniem jest synteza surowych danych z wielu testÃ³w psychometrycznych w jeden, spÃ³jny i uÅ¼yteczny profil strategiczny.

SUROWE DANE PSYCHOMETRYCZNE:
{json.dumps(cumulative_psychology, ensure_ascii=False, indent=2)}

ARCHETYP KLIENTA:
{json.dumps(customer_archetype, ensure_ascii=False, indent=2)}

PEWNOÅšÄ† ANALIZY: {confidence}%

ZADANIE: Przeanalizuj poniÅ¼sze dane. Zidentyfikuj kluczowe wzorce, synergie i potencjalne sprzecznoÅ›ci. 
Twoim celem jest stworzenie skondensowanego "DNA Klienta", ktÃ³re posÅ‚uÅ¼y strategowi sprzedaÅ¼y do podjÄ™cia dalszych dziaÅ‚aÅ„.

ZwrÃ³Ä‡ odpowiedÅº WYÅÄ„CZNIE w formacie JSON o nastÄ™pujÄ…cej strukturze:
{{
    "holistic_summary": "Jednozdaniowe, esencjonalne podsumowanie klienta, np. 'Analityczny CFO motywowany bezpieczeÅ„stwem i kontrolÄ…, nieufny wobec emocjonalnych argumentÃ³w.'",
    "main_drive": "GÅ‚Ã³wny, podÅ›wiadomy motor napÄ™dowy klienta, np. 'Unikanie ryzyka i zapewnienie bezpieczeÅ„stwa'",
    "communication_style": {{
        "recommended_tone": "np. 'Formalny, oparty na danych, zwiÄ™zÅ‚y'",
        "keywords_to_use": ["np. 'dowÃ³d', 'gwarancja', 'efektywnoÅ›Ä‡', 'plan'"],
        "keywords_to_avoid": ["np. 'uczucie', 'wyobraÅº sobie', 'zaufaj mi'"]
    }},
    "key_levers": ["OdwoÅ‚anie do jego statusu eksperta", "PodkreÅ›lenie bezpieczeÅ„stwa inwestycji"],
    "red_flags": ["Pospieszania decyzji", "Stosowania nieformalnego jÄ™zyka", "PodwaÅ¼ania jego wiedzy"],
    "missing_data_gaps": "Jakich kluczowych informacji brakuje, aby ten profil byÅ‚ peÅ‚niejszy? SformuÅ‚uj to jako cel dla sprzedawcy.",
    "confidence": {confidence}
}}
"""
            
            # WywoÅ‚aj AI z dedykowanym promptem  
            logger.info("ğŸ¤– [HOLISTIC SYNTHESIS] WysyÅ‚am prompt do AI...")
            ai_response = await self._call_llm_with_retry(
                system_prompt="JesteÅ› world-class business psychologist generujÄ…cym DNA Klienta.",
                user_prompt=synthesis_prompt
            )
            
            # Parsuj odpowiedÅº AI
            parsed_synthesis = self._parse_holistic_synthesis_response(ai_response)
            if not parsed_synthesis:
                logger.warning("âš ï¸ [HOLISTIC SYNTHESIS] AI parsing failed, uÅ¼ywam fallback")
                return self._create_fallback_holistic_profile(raw_psychology_profile)
            
            # Dodaj metadata
            parsed_synthesis['analysis_timestamp'] = datetime.now().isoformat()
            parsed_synthesis['source_confidence'] = confidence
            parsed_synthesis['cache_hit'] = False
            
            # ğŸš€ PRIORYTET 4: Zapisz do cache
            self._save_to_cache(self._holistic_synthesis_cache, cache_key, parsed_synthesis)
            
            logger.info(f"âœ… [HOLISTIC SYNTHESIS] DNA Klienta wygenerowane i cached! Drive: {parsed_synthesis.get('main_drive', 'Unknown')}")
            
            return parsed_synthesis
            
        except Exception as e:
            logger.error(f"âŒ [HOLISTIC SYNTHESIS] BÅ‚Ä…d podczas syntezy: {e}")
            return self._create_fallback_holistic_profile(raw_psychology_profile)

    def _parse_holistic_synthesis_response(self, ai_response: str) -> Optional[Dict[str, Any]]:
        """Parsuje odpowiedÅº AI z syntezÄ… holistycznÄ…"""
        try:
            # ZnajdÅº JSON w odpowiedzi
            start_idx = ai_response.find('{')
            end_idx = ai_response.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                logger.warning("âš ï¸ [HOLISTIC SYNTHESIS PARSE] Brak JSON w odpowiedzi AI")
                return None
                
            json_str = ai_response[start_idx:end_idx]
            parsed_data = json.loads(json_str)
            
            # Walidacja podstawowych pÃ³l
            required_fields = ['holistic_summary', 'main_drive', 'communication_style']
            for field in required_fields:
                if field not in parsed_data:
                    logger.warning(f"âš ï¸ [HOLISTIC SYNTHESIS PARSE] Brakuje pola: {field}")
                    return None
                    
            logger.info("âœ… [HOLISTIC SYNTHESIS PARSE] PomyÅ›lnie sparsowano DNA Klienta")
            return parsed_data
            
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸ [HOLISTIC SYNTHESIS PARSE] JSON decode error: {e}")
            return None
        except Exception as e:
            logger.warning(f"âš ï¸ [HOLISTIC SYNTHESIS PARSE] Unexpected error: {e}")
            return None

    def _create_fallback_holistic_profile(self, raw_psychology_profile: Dict[str, Any]) -> Dict[str, Any]:
        """Tworzy fallback DNA Klienta gdy AI nie jest dostÄ™pny"""
        archetype = raw_psychology_profile.get('customer_archetype', {})
        confidence = raw_psychology_profile.get('psychology_confidence', 0)
        
        return {
            'holistic_summary': f"Klient {archetype.get('archetype_name', 'o nieokreÅ›lonym profilu')} - analiza w toku",
            'main_drive': archetype.get('motivation', 'Potrzeba wiÄ™cej danych aby okreÅ›liÄ‡ gÅ‚Ã³wnÄ… motywacjÄ™'),
            'communication_style': {
                'recommended_tone': archetype.get('communication_style', 'OstroÅ¼ny, wywaÅ¼ony'),
                'keywords_to_use': ['informacje', 'korzyÅ›ci', 'rozwiÄ…zanie'],
                'keywords_to_avoid': ['presja', 'poÅ›piech', 'ryzyko']
            },
            'key_levers': ['Zbieranie informacji', 'Budowanie zaufania'],
            'red_flags': ['Zbyt szybkie podejmowanie decyzji'],
            'missing_data_gaps': 'Potrzeba wiÄ™cej interakcji aby okreÅ›liÄ‡ szczegÃ³Å‚owy profil',
            'confidence': max(confidence, 10),
            'analysis_timestamp': datetime.now().isoformat(),
            'is_fallback': True
        }

    def _create_fallback_sales_indicators(self) -> Dict[str, Any]:
        """Tworzy fallback wskaÅºniki sprzedaÅ¼owe gdy AI nie jest dostÄ™pny"""
        return {
            'purchase_temperature': {
                'value': 50,
                'temperature_level': 'warm',
                'rationale': 'Fallback - brak danych AI do oceny temperatury',
                'strategy': 'Zbieraj wiÄ™cej informacji o intencjach zakupowych',
                'confidence': 10
            },
            'customer_journey_stage': {
                'value': 'consideration',
                'progress_percentage': 40,
                'next_stage': 'evaluation',
                'rationale': 'Fallback - szacowany Å›redni etap procesu',
                'strategy': 'Kontynuuj budowanie Å›wiadomoÅ›ci korzyÅ›ci',
                'confidence': 10
            },
            'churn_risk': {
                'value': 50,
                'risk_level': 'medium',
                'risk_factors': ['Brak wystarczajÄ…cych danych'],
                'rationale': 'Fallback - Å›rednie ryzyko przy braku informacji',
                'strategy': 'Monitoruj zaangaÅ¼owanie i zadawaj pytania weryfikujÄ…ce',
                'confidence': 10
            },
            'sales_potential': {
                'value': 200000.0,
                'probability': 40,
                'estimated_timeframe': '4-8 tygodni',
                'rationale': 'Fallback - szacunek Å›redni dla klientÃ³w Tesla',
                'strategy': 'Zbieraj informacje o budÅ¼ecie i procesie decyzyjnym',
                'confidence': 10
            },
            'analysis_timestamp': datetime.now().isoformat(),
            'is_fallback': True
        }


# === GLOBAL AI SERVICE INSTANCE ===
# Zainicjalizuj globalnÄ… instancjÄ™ - utworzona po imporcie QdrantService
ai_service = None  # Zainicjalizowane przez dependency injection

# === HELPER FUNCTIONS ===

async def generate_sales_analysis(
    user_input: str,
    client_profile: Dict[str, Any],
    session_history: List[Dict[str, Any]],
    session_context: Dict[str, Any],
    session_psychology: Optional[Dict[str, Any]] = None,  # DEPRECATED v4.0
    holistic_profile: Optional[Dict[str, Any]] = None,     # NOWY v4.0: DNA Klienta
    sales_indicators: Optional[Dict[str, Any]] = None,     # NOWY v4.1: PRIORYTET 3
    customer_archetype: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    ğŸ§ âš¡ ULTRA MÃ“ZG v4.1: Enhanced helper function z sales indicators integration
    
    WywoÅ‚uje gÅ‚Ã³wnÄ… metodÄ™ generate_analysis z AIService z integracjÄ… sales indicators.
    
    Args:
        sales_indicators: Wygenerowane wskaÅºniki sprzedaÅ¼owe z DNA Klienta
        holistic_profile: DNA Klienta z holistycznej syntezy
        ... (pozostaÅ‚e argumenty jak wczeÅ›niej)
    
    Returns:
        dict: AI response wzbogacone o sales indicators
    """
    try:
        # WywoÅ‚aj gÅ‚Ã³wnÄ… analizÄ™ AI
        ai_response = await ai_service.generate_analysis(
            user_input=user_input,
            client_profile=client_profile,
            session_history=session_history,
            session_context=session_context,
            holistic_profile=holistic_profile
        )
        
        # ğŸ“Š PRIORYTET 3: DoÅ‚Ä…cz sales indicators do odpowiedzi
        if sales_indicators:
            ai_response['sales_indicators'] = sales_indicators
            logger.info(f"ğŸ“Š [GENERATE SALES ANALYSIS] Sales indicators doÅ‚Ä…czone do AI response")
        
        return ai_response
        
    except Exception as e:
        logger.error(f"âŒ [GENERATE SALES ANALYSIS] BÅ‚Ä…d: {e}")
        # Fallback response z sales indicators jeÅ›li dostÄ™pne
        fallback_response = {
            "main_analysis": "WystÄ…piÅ‚ problem z analizÄ…. Kontynuuj rozmowÄ™.",
            "suggested_actions": [{"action": "Zbieraj informacje", "reasoning": "Potrzebne wiÄ™cej kontekstu"}],
            "quick_response": "Rozumiem. Czy moÅ¼e Pan powiedzieÄ‡ wiÄ™cej na ten temat?",
            "is_fallback": True,
            "error_reason": str(e)
        }
        
        if sales_indicators:
            fallback_response['sales_indicators'] = sales_indicators
            
        return fallback_response
