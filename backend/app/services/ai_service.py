"""
AI Service - Integracja z modelem jƒôzykowym (LLM) poprzez Ollama
"""
import json
import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging

import ollama
from ollama import Client
from pydantic import ValidationError

from app.schemas.interaction import InteractionResponse
from app.models.domain import Client as DomainClient, Session, Interaction
from app.core.config import settings

logger = logging.getLogger(__name__)


class AIService:
    """
    Serwis AI dla generowania inteligentnych analiz sprzeda≈ºowych
    Integruje z modelem gpt-oss:120b poprzez Ollama Turbo Cloud
    
    Konfiguracja:
    - Host: https://ollama.com (chmura z akceleracjƒÖ sprzƒôtowƒÖ)
    - Autoryzacja: Bearer token z OLLAMA_API_KEY
    - Model: gpt-oss:120b
    """
    
    def __init__(self):
        self.model_name = "gpt-oss:120b"  # POPRAWKA: Dwukropek zamiast my≈õlnik
        self.max_retries = 3
        self.timeout_seconds = 60

        # POPRAWNA IMPLEMENTACJA KLIENTA
        try:
            if not settings.OLLAMA_API_KEY:
                raise ValueError("OLLAMA_API_KEY is not set in the environment.")

            self.client = Client(
                host="https://ollama.com",
                headers={'Authorization': f'Bearer {settings.OLLAMA_API_KEY}'}
            )
            logger.info("‚úÖ Klient Ollama Turbo zosta≈Ç pomy≈õlnie skonfigurowany.")
        except Exception as e:
            logger.error(f"‚ùå KRYTYCZNY B≈ÅƒÑD: Nie mo≈ºna skonfigurowaƒá klienta Ollama Turbo: {e}")
            self.client = None
    
    async def generate_analysis(
        self,
        user_input: str,
        client_profile: Dict[str, Any],
        session_history: List[Dict[str, Any]],
        session_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generuj inteligentnƒÖ analizƒô sprzeda≈ºowƒÖ dla danej interakcji
        
        Args:
            user_input: Wej≈õcie od sprzedawcy (obserwacje, pytania klienta)
            client_profile: Profil klienta (archetyp, tagi, notatki)
            session_history: Historia ostatnich interakcji w sesji
            session_context: Dodatkowy kontekst sesji
            
        Returns:
            S≈Çownik z pe≈ÇnƒÖ analizƒÖ zgodnƒÖ z InteractionResponse schema
            
        Raises:
            Exception: Gdy nie uda≈Ço siƒô wygenerowaƒá odpowiedzi
        """
        start_time = datetime.now()
        
        try:
            # Zbuduj prompt systemowy
            system_prompt = self._build_system_prompt(
                client_profile=client_profile,
                session_history=session_history,
                session_context=session_context or {}
            )
            
            # Zbuduj prompt u≈ºytkownika
            user_prompt = self._build_user_prompt(user_input)
            
            logger.info(f"ü§ñ AI Service: Wysy≈Çanie zapytania do modelu {self.model_name}")
            logger.debug(f"User input: '{user_input[:100]}...'")
            
            # Wywo≈Çaj model LLM z retry logic
            response = await self._call_llm_with_retry(
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
            
            # Parsuj odpowied≈∫ JSON
            analysis = self._parse_llm_response(response)
            
            # Oblicz czas przetwarzania
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            
            # Dodaj metadata
            analysis["processing_time_ms"] = int(processing_time)
            analysis["model_used"] = self.model_name
            analysis["timestamp"] = datetime.now().isoformat()
            
            logger.info(f"‚úÖ AI Service: Analiza wygenerowana w {processing_time:.0f}ms")
            logger.debug(f"Confidence level: {analysis.get('confidence_level', 'N/A')}")
            
            return analysis
            
        except Exception as e:
            logger.error(f"‚ùå AI Service: B≈ÇƒÖd podczas generowania analizy: {e}")
            
            # Zwr√≥ƒá fallback response
            return self._create_fallback_response(user_input, str(e))
    
    def _build_system_prompt(
        self,
        client_profile: Dict[str, Any],
        session_history: List[Dict[str, Any]],
        session_context: Dict[str, Any]
    ) -> str:
        """
        Zbuduj dynamiczny prompt systemowy dla LLM
        """
        # Podstawowy kontekst roli
        system_prompt = """Jeste≈õ EKSPERTEM SPRZEDA≈ªY SAMOCHOD√ìW ELEKTRYCZNYCH i doradcƒÖ AI dla sprzedawc√≥w Tesla.

TwojƒÖ misjƒÖ jest analizowanie sytuacji sprzeda≈ºowej i dostarczanie PRECYZYJNYCH, PRAKTYCZNYCH rad, kt√≥re pomogƒÖ sprzedawcy zamknƒÖƒá transakcjƒô.

INSTRUKCJE KLUCZOWE:
1. Analizuj psychologiƒô klienta na podstawie jego archetypu
2. Identyfikuj sygna≈Çy kupna i sygna≈Çy ryzyka
3. Sugeruj KONKRETNE akcje, kt√≥re sprzedawca mo≈ºe natychmiast podjƒÖƒá
4. Przewiduj zastrze≈ºenia i przygotuj odpowiedzi
5. Oce≈Ñ sentiment i potencja≈Ç na skali 1-10

"""
        
        # Dodaj profil klienta
        if client_profile:
            system_prompt += f"""
PROFIL KLIENTA:
- Alias: {client_profile.get('alias', 'Nieznany')}
- Archetyp: {client_profile.get('archetype', 'Nieznany')} 
- Tagi profilujƒÖce: {', '.join(client_profile.get('tags', []))}
- Notatki analityczne: {client_profile.get('notes', 'Brak notatek')}

"""
        
        # Dodaj kontekst sesji
        if session_context:
            session_type = session_context.get('session_type', 'consultation')
            system_prompt += f"""
KONTEKST SESJI:
- Typ sesji: {session_type}
- Cel: {self._get_session_goal(session_type)}

"""
        
        # Dodaj historiƒô interakcji
        if session_history:
            system_prompt += """
HISTORIA SESJI (ostatnie interakcje):
"""
            for i, interaction in enumerate(session_history[-3:], 1):  # Ostatnie 3 interakcje
                timestamp = interaction.get('timestamp', 'nieznany czas')
                user_input = interaction.get('user_input', '')
                confidence = interaction.get('confidence_score', 'N/A')
                
                system_prompt += f"""
{i}. [{timestamp}] Sprzedawca: "{user_input[:200]}..."
   (Poprzednia pewno≈õƒá AI: {confidence}%)

"""
        
        # Instrukcje wyj≈õciowe z nowymi zasadami
        system_prompt += """
TWOJE NARZƒòDZIA ANALITYCZNE (Frameworki):
- Psychologia sprzeda≈ºy Tesla (archetypy klient√≥w)
- Analiza sygna≈Ç√≥w kupna i oporu
- Strategiczne zarzƒÖdzanie zastrze≈ºeniami  
- Budowanie warto≈õci emocjonalnej i logicznej
- Timing i pilno≈õƒá w procesie decyzyjnym

KLUCZOWE ZASADY GENEROWANIA ODPOWIEDZI:
1. Quick Response (Odpowied≈∫ Holistyczna): GenerujƒÖc pole "quick_response", przeanalizuj CA≈ÅƒÑ dotychczasowƒÖ historiƒô rozmowy. Odpowied≈∫ musi byƒá kr√≥tka, naturalna i sp√≥jna z ca≈Çym znanym kontekstem.
2. Pytania Pog≈ÇƒôbiajƒÖce (Odpowied≈∫ Atomowa): GenerujƒÖc listƒô "suggested_questions", skup siƒô WY≈ÅƒÑCZNIE na ostatniej, bie≈ºƒÖcej wypowiedzi/obserwacji u≈ºytkownika. Pytania muszƒÖ dotyczyƒá tego konkretnego punktu i pomagaƒá go zg≈Çƒôbiƒá.

WYMAGANY FORMAT ODPOWIEDZI:
Zwr√≥ƒá WY≈ÅƒÑCZNIE poprawny JSON zgodny z tym schematem (bez dodatkowego tekstu):

{
    "quick_response": "Kr√≥tka, naturalna odpowied≈∫ sp√≥jna z CA≈ÅƒÑ historiƒÖ rozmowy - gotowa do natychmiastowego u≈ºycia",
    
    "suggested_questions": [
        "Pytanie pog≈ÇƒôbiajƒÖce dotyczƒÖce TYLKO ostatniej wypowiedzi klienta?",
        "Drugie pytanie o ten sam konkretny punkt?",
        "Trzecie pytanie pomagajƒÖce zg≈Çƒôbiƒá tƒô ostatniƒÖ obserwacjƒô?"
    ],
    
    "main_analysis": "Holistyczna analiza ca≈Çej sytuacji na podstawie pe≈Çnej historii (2-3 zdania)",
    "client_archetype": "Zidentyfikowany archetyp na podstawie ca≈Çokszta≈Çtu interakcji",
    "confidence_level": 85,
    
    "likely_archetypes": [
        {"name": "DominujƒÖcy Archetyp", "confidence": 85, "description": "Kr√≥tki opis"},
        {"name": "Drugi Archetyp", "confidence": 45, "description": "Kr√≥tki opis"}
    ],
    
    "strategic_notes": [
        "Kluczowy insight strategiczny nr 1",
        "Kluczowy insight strategiczny nr 2", 
        "Kluczowy insight strategiczny nr 3"
    ],
    
    "suggested_actions": [
        {"action": "Konkretna akcja 1", "reasoning": "Dlaczego ta akcja"},
        {"action": "Konkretna akcja 2", "reasoning": "Dlaczego ta akcja"},
        {"action": "Konkretna akcja 3", "reasoning": "Dlaczego ta akcja"}
    ],
    
    "buy_signals": ["sygna≈Ç zakupu 1", "sygna≈Ç zakupu 2"],
    "risk_signals": ["sygna≈Ç ryzyka 1", "sygna≈Ç ryzyka 2"],
    
    "objection_handlers": {
        "potencjalny zarzut 1": "spos√≥b odpowiedzi na zarzut",
        "potencjalny zarzut 2": "spos√≥b odpowiedzi na zarzut"
    },
    
    "sentiment_score": 7,
    "potential_score": 6,
    "urgency_level": "medium",
    
    "next_best_action": "Najwa≈ºniejsza nastƒôpna akcja wynikajƒÖca z ca≈Çokszta≈Çtu analizy",
    "follow_up_timing": "Rekomendowany timing nastƒôpnego kontaktu"
}

KRYTYCZNE INSTRUKCJE WYKONANIA:
1. QUICK RESPONSE = Holistyczna: Uwzglƒôdnij ca≈ÇƒÖ historiƒô, kontekst klienta, jego archetyp i wszystkie poprzednie interakcje
2. SUGGESTED QUESTIONS = Atomowa: Ignoruj historiƒô, skup siƒô tylko na ostatniej wypowiedzi i jak jƒÖ g≈Çƒôbiej zbadaƒá
3. Pole "likely_archetypes" musi zawieraƒá 1-2 najbardziej prawdopodobne archetypy z procentowym dopasowaniem
4. Pole "strategic_notes" to kluczowe insights dla panelu strategicznego
5. Quick response ma byƒá gotowy do natychmiastowego wypowiedzenia - maksymalnie 2 zdania

PAMIƒòTAJ: Odpowiadaj TYLKO w JSON. ≈ªadnego dodatkowego tekstu przed ani po JSON!
"""
        
        return system_prompt
    
    def _build_user_prompt(self, user_input: str) -> str:
        """
        Zbuduj prompt u≈ºytkownika na podstawie jego wej≈õcia
        """
        return f"""
AKTUALNA SYTUACJA:
Sprzedawca raportuje: "{user_input}"

Przeanalizuj tƒô sytuacjƒô i dostarcz inteligentnych rekomendacji w formacie JSON.
"""
    
    def _get_session_goal(self, session_type: str) -> str:
        """
        Zwr√≥ƒá cel sesji na podstawie jej typu
        """
        goals = {
            "consultation": "Zrozumienie potrzeb klienta i zbudowanie zaanga≈ºowania",
            "follow-up": "Kontynuacja rozmowy i przesuniƒôcie w stronƒô decyzji",
            "negotiation": "Negocjacja warunk√≥w i zamkniƒôcie transakcji", 
            "demo": "Prezentacja produktu i wzbudzenie emocji",
            "closing": "Finalizacja sprzeda≈ºy i podpisanie umowy"
        }
        return goals.get(session_type, "Og√≥lne wsparcie sprzeda≈ºowe")
    
    async def _call_llm_with_retry(
        self,
        system_prompt: str,
        user_prompt: str
    ) -> str:
        """
        Wywo≈Çaj model LLM z logikƒÖ retry
        """
        last_exception = None
        
        for attempt in range(self.max_retries):
            try:
                logger.debug(f"üîÑ AI Service: Pr√≥ba {attempt + 1}/{self.max_retries}")
                
                # Wywo≈Çanie Ollama w asynchronicznym kontek≈õcie
                response = await asyncio.to_thread(
                    self._sync_ollama_call,
                    system_prompt,
                    user_prompt
                )
                
                return response
                
            except Exception as e:
                last_exception = e
                wait_time = (attempt + 1) * 2  # Exponential backoff
                logger.warning(f"‚ö†Ô∏è AI Service: Pr√≥ba {attempt + 1} nieudana: {e}")
                
                if attempt < self.max_retries - 1:
                    logger.info(f"‚è≥ Ponawiam za {wait_time} sekund...")
                    await asyncio.sleep(wait_time)
        
        # Wszystkie pr√≥by nieudane
        raise Exception(f"LLM call failed after {self.max_retries} attempts. Last error: {last_exception}")
    
    def _sync_ollama_call(self, system_prompt: str, user_prompt: str) -> str:
        """
        Synchroniczne wywo≈Çanie Ollama Cloud (uruchomione w thread)
        """
        if self.client is None:
            raise ConnectionError("Klient Ollama nie zosta≈Ç poprawnie zainicjalizowany.")

        return self.client.chat(  # <-- KLUCZOWA ZMIANA: u≈ºywamy instancji klienta
            model=self.model_name, # U≈ºywamy self.model_name (gpt-oss:120b)
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': user_prompt}
            ],
            options={
                'temperature': 0.7,
                'top_p': 0.9,
                'max_tokens': 2048
            }
        )['message']['content']
    
    def _parse_llm_response(self, llm_response: str) -> Dict[str, Any]:
        """
        Parsuj odpowied≈∫ LLM do struktury JSON
        """
        try:
            # Usu≈Ñ potencjalne bia≈Çe znaki i znajd≈∫ JSON
            cleaned_response = llm_response.strip()
            
            # Znajd≈∫ poczƒÖtek i koniec JSON (na wypadek dodatkowego tekstu)
            start_idx = cleaned_response.find('{')
            end_idx = cleaned_response.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                raise ValueError("Nie znaleziono JSON w odpowiedzi LLM")
            
            json_str = cleaned_response[start_idx:end_idx]
            
            # Parsuj JSON
            parsed_data = json.loads(json_str)
            
            # Waliduj przez Pydantic schema
            interaction_response = InteractionResponse(**parsed_data)
            
            # Zwr√≥ƒá jako dict
            return interaction_response.model_dump()
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå JSON parsing error: {e}")
            logger.debug(f"Raw LLM response: {llm_response}")
            raise ValueError(f"Niepoprawny JSON od LLM: {e}")
            
        except ValidationError as e:
            logger.error(f"‚ùå Pydantic validation error: {e}")  
            logger.debug(f"Parsed data: {parsed_data}")
            raise ValueError(f"Odpowied≈∫ LLM nie pasuje do schematu: {e}")
    
    def _create_fallback_response(self, user_input: str, error_msg: str) -> Dict[str, Any]:
        """
        Stw√≥rz fallback response gdy LLM nie dzia≈Ça
        """
        logger.warning(f"üîÑ AI Service: U≈ºywam fallback response dla: '{user_input[:50]}...'")
        
        return {
            "main_analysis": f"Analiza automatyczna: '{user_input[:100]}...' - Nie uda≈Ço siƒô po≈ÇƒÖczyƒá z AI. Sprawd≈∫ sytuacjƒô wed≈Çug standardowych procedur.",
            "client_archetype": "Nieznany (b≈ÇƒÖd AI)",
            "confidence_level": 30,
            
            "suggested_actions": [
                {"action": "Zadawaj pytania otwarte", "reasoning": "Zbieraj wiƒôcej informacji"},
                {"action": "S≈Çuchaj aktywnie", "reasoning": "Zrozum potrzeby klienta"},
                {"action": "Przedstaw korzy≈õci", "reasoning": "Buduj warto≈õƒá produktu"},
                {"action": "Zaproponuj nastƒôpny krok", "reasoning": "Utrzymaj momentum"}
            ],
            
            "buy_signals": ["zainteresowanie", "pytania szczeg√≥≈Çowe"],
            "risk_signals": ["wahanie", "brak zaanga≈ºowania"],
            
            "key_insights": [
                "AI niedostƒôpny - u≈ºyj do≈õwiadczenia",
                "Skup siƒô na budowaniu relacji",
                "Zadawaj pytania kwalifikujƒÖce"
            ],
            
            "objection_handlers": {
                "za drogo": "Poka≈º warto≈õƒá d≈ÇugoterminowƒÖ",
                "potrzebujƒô czasu": "Zapytaj o konkretne obawy"
            },
            
            "qualifying_questions": [
                "Co jest dla Pana najwa≈ºniejsze w nowym samochodzie?",
                "Jaki jest Pana bud≈ºet?",
                "Kiedy planuje Pan podjƒÖƒá decyzjƒô?"
            ],
            
            "sentiment_score": 5,
            "potential_score": 5,
            "urgency_level": "medium",
            
            "next_best_action": "Zbierz wiƒôcej informacji o potrzebach klienta",
            "follow_up_timing": "W ciƒÖgu 24-48 godzin",
            
            # Natychmiastowa odpowied≈∫ (fallback)
            "quick_response": "Rozumiem. Opowiedz mi wiƒôcej o swoich potrzebach.",
            
            # Metadata b≈Çƒôdu
            "is_fallback": True,
            "error_reason": error_msg,
            "processing_time_ms": 0,
            "model_used": f"{self.model_name} (fallback)",
            "timestamp": datetime.now().isoformat()
        }


# Singleton instance
ai_service = AIService()


# Helper funkcje dla ≈Çatwego importu
async def generate_sales_analysis(
    user_input: str,
    client_profile: Dict[str, Any],
    session_history: List[Dict[str, Any]],
    session_context: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Wygeneruj analizƒô sprzeda≈ºowƒÖ - g≈Ç√≥wna funkcja eksportowa
    """
    return await ai_service.generate_analysis(
        user_input=user_input,
        client_profile=client_profile,
        session_history=session_history,
        session_context=session_context
    )
