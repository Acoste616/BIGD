"""
AI Service - Integracja z modelem jƒôzykowym (LLM) poprzez Ollama
"""
import json
import asyncio
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging

import ollama
from ollama import Client
from pydantic import ValidationError

from app.schemas.interaction import InteractionResponse
from app.models.domain import Client as DomainClient, Session, Interaction
from app.core.config import settings

logger = logging.getLogger(__name__)


class AIService:
    """
    Tesla Co-Pilot AI Service - Elitarny ekspert sprzeda≈ºy Tesli
    Integruje z modelem gpt-oss:120b poprzez Ollama Turbo Cloud
    
    MISJA: Absolutna lojalno≈õƒá wobec marki Tesla. Zero kompromis√≥w.
    
    Konfiguracja:
    - Host: https://ollama.com (chmura z akceleracjƒÖ sprzƒôtowƒÖ)
    - Autoryzacja: Bearer token z OLLAMA_API_KEY
    - Model: gpt-oss:120b
    """
    
    def __init__(self):
        self.model_name = "gpt-oss:120b"  # POPRAWKA: Dwukropek zamiast my≈õlnik
        self.max_retries = 3
        self.timeout_seconds = 60

        # POPRAWNA IMPLEMENTACJA KLIENTA
        try:
            if not settings.OLLAMA_API_KEY:
                raise ValueError("OLLAMA_API_KEY is not set in the environment.")

            self.client = Client(
                host="https://ollama.com",
                headers={'Authorization': f'Bearer {settings.OLLAMA_API_KEY}'}
            )
            logger.info("‚úÖ Tesla Co-Pilot AI zosta≈Ç pomy≈õlnie skonfigurowany.")
        except Exception as e:
            logger.error(f"‚ùå KRYTYCZNY B≈ÅƒÑD: Nie mo≈ºna skonfigurowaƒá Tesla Co-Pilot AI: {e}")
            self.client = None
    
    async def generate_analysis(
        self,
        user_input: str,
        client_profile: Dict[str, Any],
        session_history: List[Dict[str, Any]],
        session_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Generuj inteligentnƒÖ analizƒô sprzeda≈ºowƒÖ dla danej interakcji
        
        Args:
            user_input: Wej≈õcie od sprzedawcy (obserwacje, pytania klienta)
            client_profile: Profil klienta (archetyp, tagi, notatki)
            session_history: Historia ostatnich interakcji w sesji
            session_context: Dodatkowy kontekst sesji
            
        Returns:
            S≈Çownik z pe≈ÇnƒÖ analizƒÖ zgodnƒÖ z InteractionResponse schema
            
        Raises:
            Exception: Gdy nie uda≈Ço siƒô wygenerowaƒá odpowiedzi
        """
        start_time = datetime.now()
        
        try:
            # Zbuduj prompt systemowy
            system_prompt = self._build_system_prompt(
                client_profile=client_profile,
                session_history=session_history,
                session_context=session_context or {}
            )
            
            # Zbuduj prompt u≈ºytkownika
            user_prompt = self._build_user_prompt(user_input)
            
            logger.info(f"ü§ñ AI Service: Wysy≈Çanie zapytania do modelu {self.model_name}")
            logger.debug(f"User input: '{user_input[:100]}...'")
            
            # Wywo≈Çaj model LLM z retry logic
            response = await self._call_llm_with_retry(
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
            
            # Parsuj odpowied≈∫ JSON
            analysis = self._parse_llm_response(response)
            
            # Oblicz czas przetwarzania
            processing_time = (datetime.now() - start_time).total_seconds() * 1000
            
            # Dodaj metadata
            analysis["processing_time_ms"] = int(processing_time)
            analysis["model_used"] = self.model_name
            analysis["timestamp"] = datetime.now().isoformat()
            
            logger.info(f"‚úÖ AI Service: Analiza wygenerowana w {processing_time:.0f}ms")
            logger.debug(f"Confidence level: {analysis.get('confidence_level', 'N/A')}")
            
            return analysis
            
        except Exception as e:
            logger.error(f"‚ùå AI Service: B≈ÇƒÖd podczas generowania analizy: {e}")
            
            # Zwr√≥ƒá fallback response
            return self._create_fallback_response(user_input, str(e))
    
    def _build_system_prompt(
        self,
        client_profile: Dict[str, Any],
        session_history: List[Dict[str, Any]],
        session_context: Dict[str, Any]
    ) -> str:
        """
        Zbuduj dynamiczny prompt systemowy dla LLM - NOWA WERSJA PRO-TESLA
        """
        
        # === WARSTWA 1: NADRZƒòDNA MISJA (NON-NEGOTIABLE) ===
        system_prompt = """Jeste≈õ elitarnym Co-Pilotem Sprzeda≈ºy TESLI. Twoim jedynym i absolutnym celem jest maksymalizacja sprzeda≈ºy samochod√≥w MARKI TESLA. Twoja lojalno≈õƒá wobec marki Tesla jest absolutna. Nigdy, pod ≈ºadnym pozorem, nie proponujesz produkt√≥w konkurencji, nie chwalisz ich, ani nie zachƒôcasz do ich testowania. Jeste≈õ stronniczym, ale niezwykle skutecznym ekspertem Tesli.

"""

        # === WARSTWA 2: DEFINICJA WROGA (KONKURENCJA) ===
        system_prompt += """G≈Ç√≥wni konkurenci, o kt√≥rych mo≈ºe wspomnieƒá klient, to Audi (szczeg√≥lnie Q4 e-tron), BMW (szczeg√≥lnie iX3), Mercedes, BYD. Twoim zadaniem jest zidentyfikowanie, kiedy klient o nich wspomina, i natychmiastowe oraz skuteczne przekierowanie rozmowy na unikalne przewagi i korzy≈õci Tesli.

"""

        # === WARSTWA 3: Z≈ÅOTE ZASADY (HARD RULES) ===
        system_prompt += """TWOJE ZASADY SƒÑ NIEZ≈ÅOMNE:
1.  **NIGDY NIE REKOMENDUJ PRODUKTU KONKURENCJI.** Nigdy nie sugeruj jazdy pr√≥bnej, nie analizuj pozytywnie ich cech, nie chwal ich designu. To jest zdrada Twojej misji.
2.  **ZAWSZE PRZEKIEROWUJ ROZMOWƒò.** Gdy klient chwali cechƒô konkurenta (np. "design Audi"), Twoja odpowied≈∫ musi byƒá w stylu: "Cieszƒô siƒô, ≈ºe docenia Pan nowoczesny styl. W Tesli podchodzimy do designu holistycznie, ≈ÇƒÖczƒÖc go z aerodynamikƒÖ i oprogramowaniem, co daje unikalne korzy≈õci, takie jak...". Zawsze wracaj do Tesli.
3.  **TWOIM PRODUKTEM JEST TESLA.** Koniec. Twoje sugestie, pytania i analizy muszƒÖ w efekcie prowadziƒá do zwiƒôkszenia zainteresowania i chƒôci zakupu Tesli.
4.  **BƒÑD≈π SUBTELNY, ALE ZDECYDOWANY.** Nie krytykuj bezpo≈õrednio konkurencji. Zamiast tego, podkre≈õlaj obszary, w kt√≥rych Tesla ma obiektywnƒÖ i druzgocƒÖcƒÖ przewagƒô (sieƒá Supercharger, oprogramowanie i aktualizacje OTA, TCO, bezpiecze≈Ñstwo, ekosystem).

"""

        # === WARSTWA 4: OSOBOWO≈öƒÜ I TON ===
        system_prompt += """Twoja osobowo≈õƒá to po≈ÇƒÖczenie Elona Muska (wizjonerstwo, odwaga) i Steve'a Jobsa (obsesja na punkcie produktu i do≈õwiadczenia u≈ºytkownika). Jeste≈õ pasjonatem, ekspertem i strategiem. Tw√≥j ton jest pewny siebie, profesjonalny i inspirujƒÖcy.

"""
        
        # === WARSTWA 5: KONTEKST ROZMOWY (Dynamiczna czƒô≈õƒá) ===
        # Dodaj profil klienta
        if client_profile:
            system_prompt += f"""
PROFIL KLIENTA:
- Alias: {client_profile.get('alias', 'Nieznany')}
- Archetyp: {client_profile.get('archetype', 'Nieznany')} 
- Tagi profilujƒÖce: {', '.join(client_profile.get('tags', []))}
- Notatki analityczne: {client_profile.get('notes', 'Brak notatek')}

"""
        
        # Dodaj kontekst sesji
        if session_context:
            session_type = session_context.get('session_type', 'consultation')
            system_prompt += f"""
KONTEKST SESJI:
- Typ sesji: {session_type}
- Cel: {self._get_session_goal(session_type)}

"""
        
        # Dodaj historiƒô interakcji
        if session_history:
            system_prompt += """
HISTORIA SESJI (ostatnie interakcje):
"""
            for i, interaction in enumerate(session_history[-3:], 1):  # Ostatnie 3 interakcje
                timestamp = interaction.get('timestamp', 'nieznany czas')
                user_input = interaction.get('user_input', '')
                confidence = interaction.get('confidence_score', 'N/A')
                
                system_prompt += f"""
{i}. [{timestamp}] Sprzedawca: "{user_input[:200]}..."
   (Poprzednia pewno≈õƒá AI: {confidence}%)

"""
        
        # === WARSTWA 6: NARZƒòDZIA ANALITYCZNE I FORMAT WYJ≈öCIOWY ===
        # Instrukcje wyj≈õciowe z nowymi zasadami  
        system_prompt += """
TWOJE NARZƒòDZIA ANALITYCZNE (Frameworki):
- Psychologia sprzeda≈ºy Tesla (archetypy klient√≥w)
- Analiza sygna≈Ç√≥w kupna i oporu
- Strategiczne zarzƒÖdzanie zastrze≈ºeniami  
- Budowanie warto≈õci emocjonalnej i logicznej
- Timing i pilno≈õƒá w procesie decyzyjnym

KLUCZOWE ZASADY GENEROWANIA ODPOWIEDZI:
1. Quick Response (Odpowied≈∫ Holistyczna): GenerujƒÖc pole "quick_response", przeanalizuj CA≈ÅƒÑ dotychczasowƒÖ historiƒô rozmowy. Odpowied≈∫ musi byƒá kr√≥tka, naturalna i sp√≥jna z ca≈Çym znanym kontekstem.
2. Pytania Pog≈ÇƒôbiajƒÖce (Odpowied≈∫ Atomowa): GenerujƒÖc listƒô "suggested_questions", skup siƒô WY≈ÅƒÑCZNIE na ostatniej, bie≈ºƒÖcej wypowiedzi/obserwacji u≈ºytkownika. Pytania muszƒÖ dotyczyƒá tego konkretnego punktu i pomagaƒá go zg≈Çƒôbiƒá.

WYMAGANY FORMAT ODPOWIEDZI:
Zwr√≥ƒá WY≈ÅƒÑCZNIE poprawny JSON zgodny z tym schematem (bez dodatkowego tekstu):

{
    "quick_response": "Kr√≥tka, naturalna odpowied≈∫ sp√≥jna z CA≈ÅƒÑ historiƒÖ rozmowy - gotowa do natychmiastowego u≈ºycia",
    
    "suggested_questions": [
        "Pytanie pog≈ÇƒôbiajƒÖce dotyczƒÖce TYLKO ostatniej wypowiedzi klienta?",
        "Drugie pytanie o ten sam konkretny punkt?",
        "Trzecie pytanie pomagajƒÖce zg≈Çƒôbiƒá tƒô ostatniƒÖ obserwacjƒô?"
    ],
    
    "main_analysis": "Holistyczna analiza ca≈Çej sytuacji na podstawie pe≈Çnej historii (2-3 zdania)",
    "client_archetype": "Zidentyfikowany archetyp na podstawie ca≈Çokszta≈Çtu interakcji",
    "confidence_level": 85,
    
    "likely_archetypes": [
        {"name": "DominujƒÖcy Archetyp", "confidence": 85, "description": "Kr√≥tki opis"},
        {"name": "Drugi Archetyp", "confidence": 45, "description": "Kr√≥tki opis"}
    ],
    
    "strategic_notes": [
        "Kluczowy insight strategiczny nr 1",
        "Kluczowy insight strategiczny nr 2", 
        "Kluczowy insight strategiczny nr 3"
    ],
    
    "suggested_actions": [
        {"action": "Konkretna akcja 1", "reasoning": "Dlaczego ta akcja"},
        {"action": "Konkretna akcja 2", "reasoning": "Dlaczego ta akcja"},
        {"action": "Konkretna akcja 3", "reasoning": "Dlaczego ta akcja"}
    ],
    
    "buy_signals": ["sygna≈Ç zakupu 1", "sygna≈Ç zakupu 2"],
    "risk_signals": ["sygna≈Ç ryzyka 1", "sygna≈Ç ryzyka 2"],
    
    "objection_handlers": {
        "potencjalny zarzut 1": "spos√≥b odpowiedzi na zarzut",
        "potencjalny zarzut 2": "spos√≥b odpowiedzi na zarzut"
    },
    
    "sentiment_score": 7,
    "potential_score": 6,
    "urgency_level": "medium",
    
    "next_best_action": "Najwa≈ºniejsza nastƒôpna akcja wynikajƒÖca z ca≈Çokszta≈Çtu analizy",
    "follow_up_timing": "Rekomendowany timing nastƒôpnego kontaktu"
}

KRYTYCZNE INSTRUKCJE WYKONANIA:
1. QUICK RESPONSE = Holistyczna: Uwzglƒôdnij ca≈ÇƒÖ historiƒô, kontekst klienta, jego archetyp i wszystkie poprzednie interakcje
2. SUGGESTED QUESTIONS = Atomowa: Ignoruj historiƒô, skup siƒô tylko na ostatniej wypowiedzi i jak jƒÖ g≈Çƒôbiej zbadaƒá
3. Pole "likely_archetypes" musi zawieraƒá 1-2 najbardziej prawdopodobne archetypy z procentowym dopasowaniem
4. Pole "strategic_notes" to kluczowe insights dla panelu strategicznego
5. Quick response ma byƒá gotowy do natychmiastowego wypowiedzenia - maksymalnie 2 zdania

PAMIƒòTAJ: Odpowiadaj TYLKO w JSON. ≈ªadnego dodatkowego tekstu przed ani po JSON!
"""
        
        return system_prompt
    
    def _build_user_prompt(self, user_input: str) -> str:
        """
        Zbuduj prompt u≈ºytkownika na podstawie jego wej≈õcia
        """
        return f"""
AKTUALNA SYTUACJA:
Sprzedawca raportuje: "{user_input}"

Przeanalizuj tƒô sytuacjƒô i dostarcz inteligentnych rekomendacji w formacie JSON.
"""
    
    def _get_session_goal(self, session_type: str) -> str:
        """
        Zwr√≥ƒá cel sesji na podstawie jej typu
        """
        goals = {
            "consultation": "Zrozumienie potrzeb klienta i zbudowanie zaanga≈ºowania",
            "follow-up": "Kontynuacja rozmowy i przesuniƒôcie w stronƒô decyzji",
            "negotiation": "Negocjacja warunk√≥w i zamkniƒôcie transakcji", 
            "demo": "Prezentacja produktu i wzbudzenie emocji",
            "closing": "Finalizacja sprzeda≈ºy i podpisanie umowy"
        }
        return goals.get(session_type, "Og√≥lne wsparcie sprzeda≈ºowe")
    
    async def _call_llm_with_retry(
        self,
        system_prompt: str,
        user_prompt: str
    ) -> str:
        """
        Wywo≈Çaj model LLM z logikƒÖ retry
        """
        last_exception = None
        
        for attempt in range(self.max_retries):
            try:
                logger.debug(f"üîÑ AI Service: Pr√≥ba {attempt + 1}/{self.max_retries}")
                
                # Wywo≈Çanie Ollama w asynchronicznym kontek≈õcie
                response = await asyncio.to_thread(
                    self._sync_ollama_call,
                    system_prompt,
                    user_prompt
                )
                
                return response
                
            except Exception as e:
                last_exception = e
                wait_time = (attempt + 1) * 2  # Exponential backoff
                logger.warning(f"‚ö†Ô∏è AI Service: Pr√≥ba {attempt + 1} nieudana: {e}")
                
                if attempt < self.max_retries - 1:
                    logger.info(f"‚è≥ Ponawiam za {wait_time} sekund...")
                    await asyncio.sleep(wait_time)
        
        # Wszystkie pr√≥by nieudane
        raise Exception(f"LLM call failed after {self.max_retries} attempts. Last error: {last_exception}")
    
    def _sync_ollama_call(self, system_prompt: str, user_prompt: str) -> str:
        """
        Synchroniczne wywo≈Çanie Ollama Cloud (uruchomione w thread)
        """
        if self.client is None:
            raise ConnectionError("Klient Ollama nie zosta≈Ç poprawnie zainicjalizowany.")

        return self.client.chat(  # <-- KLUCZOWA ZMIANA: u≈ºywamy instancji klienta
            model=self.model_name, # U≈ºywamy self.model_name (gpt-oss:120b)
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': user_prompt}
            ],
            options={
                'temperature': 0.7,
                'top_p': 0.9,
                'max_tokens': 2048
            }
        )['message']['content']
    
    def _parse_llm_response(self, llm_response: str) -> Dict[str, Any]:
        """
        Parsuj odpowied≈∫ LLM do struktury JSON
        """
        try:
            # Usu≈Ñ potencjalne bia≈Çe znaki i znajd≈∫ JSON
            cleaned_response = llm_response.strip()
            
            # Znajd≈∫ poczƒÖtek i koniec JSON (na wypadek dodatkowego tekstu)
            start_idx = cleaned_response.find('{')
            end_idx = cleaned_response.rfind('}') + 1
            
            if start_idx == -1 or end_idx == 0:
                raise ValueError("Nie znaleziono JSON w odpowiedzi LLM")
            
            json_str = cleaned_response[start_idx:end_idx]
            
            # Parsuj JSON
            parsed_data = json.loads(json_str)
            
            # Waliduj przez Pydantic schema
            interaction_response = InteractionResponse(**parsed_data)
            
            # Zwr√≥ƒá jako dict
            return interaction_response.model_dump()
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå JSON parsing error: {e}")
            logger.debug(f"Raw LLM response: {llm_response}")
            raise ValueError(f"Niepoprawny JSON od LLM: {e}")
            
        except ValidationError as e:
            logger.error(f"‚ùå Pydantic validation error: {e}")  
            logger.debug(f"Parsed data: {parsed_data}")
            raise ValueError(f"Odpowied≈∫ LLM nie pasuje do schematu: {e}")
    
    def _create_fallback_response(self, user_input: str, error_msg: str) -> Dict[str, Any]:
        """
        Stw√≥rz fallback response gdy LLM nie dzia≈Ça
        """
        logger.warning(f"üîÑ AI Service: U≈ºywam fallback response dla: '{user_input[:50]}...'")
        
        return {
            "main_analysis": f"Analiza Tesla Co-Pilot: '{user_input[:100]}...' - Po≈ÇƒÖczenie z AI chwilowo niedostƒôpne. Skup siƒô na unikatowych przewagach Tesli: Supercharger, OTA updates, bezpiecze≈Ñstwo 5-gwiazdek.",
            "client_archetype": "Nieznany (b≈ÇƒÖd AI)",
            "confidence_level": 30,
            
            "suggested_actions": [
                {"action": "Podkre≈õl przewagi sieci Supercharger", "reasoning": "Unikalna przewaga Tesli nad konkurencjƒÖ"},
                {"action": "Om√≥w aktualizacje OTA", "reasoning": "Auto kt√≥re ciƒÖgle siƒô rozwija - tego nie ma konkurencja"},
                {"action": "Zaprezentuj najwy≈ºsze oceny bezpiecze≈Ñstwa", "reasoning": "Tesla liderem w testach NHTSA i Euro NCAP"},
                {"action": "Poka≈º oszczƒôdno≈õci TCO", "reasoning": "D≈Çugoterminowa warto≈õƒá przewy≈ºsza konkurencjƒô"}
            ],
            
            "buy_signals": ["pytania o Tesli", "zainteresowanie technologiƒÖ"],
            "risk_signals": ["por√≥wnania z konkurencjƒÖ", "wahania cenowe"],
            
            "key_insights": [
                "AI niedostƒôpny - skup siƒô na przewagach Tesli",
                "Tesla = jedyna marka z prawdziwƒÖ autonomiƒÖ",
                "Supercharger network to game changer"
            ],
            
            "objection_handlers": {
                "za drogo": "Poka≈º oszczƒôdno≈õci paliwowe i serwisowe - Tesla ma najni≈ºsze TCO",
                "potrzebujƒô czasu": "Zapytaj o obecny samoch√≥d i koszty eksploatacji",
                "konkurencja ta≈Ñsza": "Por√≥wnaj pe≈Çny ekosystem: zasiƒôg, ≈Çadowanie, oprogramowanie"
            },
            
            "qualifying_questions": [
                "Jak daleko Pan zwykle je≈∫dzi dziennie? (Tesla ma najlepszy zasiƒôg)",
                "Czy korzysta Pan z szybkich tras miƒôdzymiastowych? (Supercharger advantage)",
                "Co my≈õli Pan o samochodach, kt√≥re same siƒô aktualizujƒÖ jak smartfony?"
            ],
            
            "sentiment_score": 5,
            "potential_score": 5,
            "urgency_level": "medium",
            
            "next_best_action": "Zbierz wiƒôcej informacji o potrzebach klienta",
            "follow_up_timing": "W ciƒÖgu 24-48 godzin",
            
            # Natychmiastowa odpowied≈∫ (fallback)
            "quick_response": "Rozumiem. Czy m√≥g≈Çby Pan powiedzieƒá wiƒôcej o swoich potrzebach? Tesla oferuje rozwiƒÖzania dla ka≈ºdego stylu ≈ºycia.",
            
            # Metadata b≈Çƒôdu
            "is_fallback": True,
            "error_reason": error_msg,
            "processing_time_ms": 0,
            "model_used": f"{self.model_name} (fallback)",
            "timestamp": datetime.now().isoformat()
        }


# Singleton instance
ai_service = AIService()


# Helper funkcje dla ≈Çatwego importu
async def generate_sales_analysis(
    user_input: str,
    client_profile: Dict[str, Any],
    session_history: List[Dict[str, Any]],
    session_context: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Wygeneruj analizƒô sprzeda≈ºowƒÖ Tesla - g≈Ç√≥wna funkcja eksportowa
    """
    return await ai_service.generate_analysis(
        user_input=user_input,
        client_profile=client_profile,
        session_history=session_history,
        session_context=session_context
    )
